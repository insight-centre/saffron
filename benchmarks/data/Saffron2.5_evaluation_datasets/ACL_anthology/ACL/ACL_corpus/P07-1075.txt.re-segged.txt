Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics , pages 592?599,
Prague , Czech Republic , June 2007. c?2007 Association for Computational Linguistics
A Multi-resolution Framework for Information Extraction from Free Text
Mstislav Maslennikov and Tat-Seng Chua
Department of Computer Science
National University of Singapore
{maslenni,chuats}@comp.nus.edu.sg
Abstract
Extraction of relations between entities is an important part of Information Extraction on free text . Previous methods are mostly based on statistical correlation and dependency relations between entities . This paper re-examines the problem at the multiresolution layers of phrase , clause and sentence using dependency and discourse relations . Our multiresolution framework ARE ( Anchor and Relation ) uses clausal relations in 2 ways : 1) to filter noisy dependency paths ; and 2) to increase reliability of dependency path extraction . The resulting system outperforms the previous approaches by 3%, 7%, 4% on MUC4,
MUC6 and ACE RDC domains respectively.
1 Introduction
Information Extraction ( IE ) is the task of identifying information in texts and converting it into a predefined format . The possible types of information include entities , relations or events . In this paper , we follow the IE tasks as defined by the conferences MUC4, MUC6 and ACE RDC : slot-based extraction , template filling and relation extraction , respectively.
Previous approaches to IE relied on cooccurrence ( Xiao et al , 2004) and dependency ( Zhang et al , 2006) relations between entities.
These relations enable us to make reliable extraction of correct entities/relations at the level of a single clause . However , Maslennikov et al (2006) reported that the increase of relation path length will lead to considerable decrease in performance.
In most cases , this decrease in performance occurs because entities may belong to different clauses.
Since clauses in a sentence are connected by clausal relations ( Halliday and Hasan , 1976), it is thus important to perform discourse analysis of a sentence.
Discourse analysis may contribute to IE in several ways . First , Taboada and Mann (2005) reported that discourse analysis helps to decompose long sentences into clauses . Therefore , it helps to distinguish relevant clauses from nonrelevant ones . Second , Miltsakaki (2003) stated that entities in subordinate clauses are less salient . Third , the knowledge of textual structure helps to interpret the meaning of entities in a text ( Grosz and Sidner 1986). As an example , consider the sentences ? ABC Co . appointed a new chairman . Additionally , the current CEO was retired ?. The word ? additionally ? connects the event in the second sentence to the entity ? ABC Co .? in the first sentence.
Fourth , Moens and De Busser (2002) reported that discourse segments tend to be in a fixed order for structured texts such as court decisions or news.
Hence , analysis of discourse order may reduce the variability of possible relations between entities.
To model these factors , we propose a multiresolution framework ARE that integrates both discourse and dependency relations at 2 levels.
ARE aims to filter noisy dependency relations from training and support their evaluation with discourse relations between entities . Additionally , we encode semantic roles of entities in order to utilize semantic relations . Evaluations on MUC4, MUC6 and ACE RDC 2003 corpora demonstrates that our approach outperforms the state-of-art systems mainly due to modeling of discourse relations.
The contribution of this paper is in applying discourse relations to supplement dependency relations in a multiresolution framework for IE . The ent clauses and thus improve the performance on long-distance dependency paths.
Section 2 describes related work , while Section 3 presents our proposed framework , including the extraction of anchor cues and various types of relations , integration of extracted relations , and complexity classification . Section 4 describes our experimental results , with the analysis of results in Section 5. Section 6 concludes the paper.
2 Related work
Recent work in IE focuses on relation-based , semantic parsing-based and discourse-based approaches . Several recent research efforts were based on modeling relations between entities . Culotta and Sorensen (2004) extracted relationships using dependency-based kernel trees in Support Vector Machines ( SVM ). They achieved an F1-measure of 63% in relation detection . The authors reported that the primary source of mistakes comes from the heterogeneous nature of non-relation instances . One possible direction to tackle this problem is to carry out further relationship classification . Maslennikov et al (2006) classified relation path between candidate entities into simple , average and hard cases . This classification is based on the length of connecting path in dependency parse tree . They reported that dependency relations are not reliable for the hard cases , which , in our opinion , need the extraction of discourse relations to supplement dependency relation paths.
Surdeanu et al (2003) applied semantic parsing to capture the predicate-argument sentence structure . They suggested that semantic parsing is useful to capture verb arguments , which may be connected by long-distance dependency paths . However , current semantic parsers such as the ASSERT are not able to recognize support verb constructions such as ? X conducted an attack on Y ? under the verb frame ? attack ? ( Pradhan et al 2004).
Hence , many useful predicate-argument structures will be missed . Moreover , semantic parsing belongs to the intra-clausal level of sentence analysis , which , as in the dependency case , will need the support of discourse analysis to bridge inter-clausal relations.
Webber et al (2002) reported that discourse structure helps to extract anaphoric relations . However , their set of grammatical rules is heuristic . Our task needs construction of an automated approach to be portable across several domains . Cimiano et al . (2005) employed a discourse-based analysis for IE . However , their approach requires a predefined domain-dependent ontology in the format of extended logical description grammar as described by Cimiano and Reely (2003). Moreover , they used discourse relations between events , whereas in our approach , discourse relations connect entities.
3 Motivation for using discourse relations Our method is based on Rhetorical Structure Theory ( RST ) by Taboada and Mann (2005). RST splits the texts into 2 parts : a ) nuclei , the most important parts of texts ; and b ) satellites , the secondary parts . We can often remove satellites without losing the meaning of text . Both nuclei and satellites are connected with discourse relations in a hierarchical structure . In our work , we use 16 classes of discourse relations between clauses : Attribution , Background , Cause , Comparison , Condition , Contrast , Elaboration , Enablement , Evaluation , Explanation , Joint , Manner-Means , Topic-Comment , Summary , Temporal , Topic-Change.
The additional 3 relations impose a tree structure : textual-organization , span and same-unit . All the discourse relation classes are potentially useful , since they encode some knowledge about textual structure . Therefore , we decide to include all of them in the learning process to learn patterns with best possible performance.
We consider two main rationales for utilizing discourse relations to IE . First , discourse relations help to narrow down the search space to the level of a single clause . For example , the sentence ?[< Soc-A1>Trudeau</>'s < Soc-A2>son </> told everyone ], [ their prime minister was his father ], [ who took him to a secret base in the arctic ] [ and let him peek through a window ].? contains 4 clauses and 7 anchor cues ( key phrases ) for the type Social , which leads to 21 possible variants.
Splitting this sentence into clauses reduces the combinations to 4 possible variants . Additionally , this reduction eliminates the long and noisy dependency paths.
Second , discourse analysis enables us to connect entities in different clauses with clausal relations.
As an example , we consider a sentence ? It?s a dark comedy about a boy named < AT-A1>Marshal </> played by Amourie Kats who discovers all kinds of ingly quiet little town </>?. In this example , we need to extract the relation ? At ? between the entities ? Marshal ? and ? a seemingly quiet little town?.
The discourse structure of this sentence is given in . Figure 1 Figure 1. Example of discourse parsing The discourse path ? Marshal <- elaboration - _ <- span - _ - elaboration -> _ - elaboration -> town ? is relatively short and captures the necessary relations . At the same time , prediction based on dependency path ? Marshal <? obj - _ <- i - _ <- fc - _ <- pnmod - _ <- pred - _ <- i - _ <- null - _ - null -> _ - rel -> _ - i -> _ - mod -> _ - pcompn -> town ? is unreliable , since the relation path is long . Thus , it is important to rely on discourse analysis in this example . In addition , we need to evaluate both the score and reliability of prediction by relation path of each type.
4 Anchors and Relations
In this section , we define the key components that we use in ARE : anchors , relation types and general architecture of our system . Some of these components are also presented in detail in our previous work ( Maslennikov et al , 2006).
4.1 Anchors
The first task in IE is to identify candidate phrases ( which we call anchor or anchor cue ) of a predefined type ( anchor type ) to fill a desired slot in an IE template . The example anchor for the phrase ? Marshal ? is shown in Figure 2.
Given a training set of sentences , we extract the anchor cues ACj = [ A1, ?, ANanch ] of type Cj using the procedures described in
Maslennikov et al (2006). The linguistic features of these anchors for the anchor types of Perpetrator , Action , Victim and Target for the MUC4 domain are given in Table 1.

Anchor types
Feature
Perpetrator_Cue ( A)
Action_Cue ( D)
Victim_Cue ( A)
Target_Cue ( A)
Lexical ( Head noun ) terrorists , individuals,
Soldiers attacked , murder,
Massacre
Mayor , general , priests bridge , house,
Ministry
Part-of-Speech Noun Verb Noun Noun
Named Entities
Soldiers ( PERSON ) - Jesuit priests ( PERSON)
WTC ( OBJECT)
Synonyms Synset 130, 166 Synset 22 Synset 68 Synset 71 Concept Class ID 2, 3 ID 9 ID 22, 43 ID 61, 48
Coreferenced entity
He -> terrorist , soldier - They -> peasants -
Clausal type Nucleus
Satellite
Nucleus,
Satellite
Nucleus,
Satellite
Nucleus,
Satellite
Argument type Arg0 , Arg1
Root Target , -,
ArgM-MNR
Arg0 , Arg1 Arg1 , ArgM-
MNR
Table 1. Linguistic features for anchor extraction Given an input phrase P from a test sentence , we need to classify if the phrase belongs to anchor cue type Cj . We calculate the entity score as : Entity_Score(P ) =? ? i * Feature_Scorei(P,Cj ) (1) where Feature_Score(P,Cj ) is a score function for a particular linguistic feature representation of type Cj , and ? i is the corresponding weight for that representation in the overall entity score . The weights are learned automatically using Expectation Maximization ( Dempster et al , 1977). The Fea-ture_Scorei(P,Cj ) is estimated from the training set as the number of slots containing the correct feature representation type versus all the slots : Feature_Scorei(P,Cj ) = #( positive slots ) / #( all slots ) (2) We classify the phrase P as belonging to an anchor type Cj when its Entity_score(P ) is above an empirically determined threshold ?. We refer to this anchor as Aj . We allow a phrase to belong to multiple anchor types and hence the anchors alone are not enough for filling templates.
4.2 Relations
To resolve the correct filling of phrase P of type Ci in a desired slot in the template , we need to consider the relations between multiple candidate phrases of related slots . To do so , we consider several types of relations between anchors : discourse , dependency and semantic relations . These relations capture the interactions between anchors and are therefore useful for tackling the paraphrasing and alignment problems ( Maslennikov et al , 2006).
Given 2 anchors Ai and Aj of anchor types Ci and Cj , we consider a relation Pathl = [ Ai , Rel1,?, Reln , Aj ] between them , such that there are no anchors between Ai and Aj . Additionally , we assume that the relations between anchors are represented in the form of a tree Tl , where l = { s , c , d } refers to
Satellite who discovers all kinds of on and scary things going on in a seemingly quiet little town.
Nucleus
It's a dark comedy about a boy
Satellite named Marshal
Nucleus played by
Amourie Kats
Nucleus Satellite span elaboration span elaboration elaboration span
Figure 2. Example of anchor
Anchor Ai
Marshal pos_NNP list_personWord
Cand_AtArg1
Minipar_obj
Arg2
Spade_Satellite respectively . We describe the nodes and edges of Tl separately for each type , because their representations are different : 1) The nodes of discourse tree Tc consist of clauses [ Clause1, ?, ClauseNcl ]; and their relation edges are obtained from the Spade system described in Soricut and Marcu (2003). This system performs RST-based parsing at the sentence level . The reported accuracy of Spade is 49% on the RSTDT corpus . To obtain a clausal path , we map each anchor Ai to its clause in Spade . If anchors Ai and Aj belong to the same clause , we assign them the relation same-clause.
es.
2) The nodes of dependency tree Td consist of words in sentences ; and their relation edges are obtained from Minipar by Lin (1997). Lin (1997) reported a parsing performance of Precision = 88.5% and Recall = 78.6% on the SU-
SANNE corpus.
3) The nodes of semantic tree Ts consist of arguments [ Arg0, ?, ArgNarg ] and targets [ Target1, ?, TargetNtarg ]. Both arguments and targets are obtained from the ASSERT parser developed by Pradhan (2004). The reported performance of ASSERT is F1=83.8% on the identification and classification task for all arguments , evaluated using PropBank and AQUAINT as the training and testing corpora , respectively . Since the relation edges have a form Targetk -> Argl , the relation path in semantic frame contains only a single relation . Therefore , we encode semantic relations as part of the anchor features.
In later parts of this paper , we consider only discourse and dependency relation paths Pathl , where l={c , d}.

Figure 3. Architecture of the system 4.3 Architecture of ARE system In order to perform IE , it is important to extract candidate entities ( anchors ) of appropriate anchor types , evaluate the relationships between them , further evaluate all possible candidate templates , and output the final template . For the case of relation extraction task , the final templates are the same as an extracted binary relation . The overall architecture of ARE is given in Figure 3.
The focus of this paper is in applying discourse relations for binary relationship evaluation.
5 Overall approach
In this section , we describe our relation-based approach to IE . We start with the evaluation of relation paths ( single relation ranking , relation path ranking ) to assess the suitability of their anchors as entities to template slots . Here we want to evaluate given a single relation or relation path , whether the two anchors are correct in filling the appropriate slots in a template . This is followed by the integration of relation paths and evaluation of templates.
5.1 Evaluation of relation path
In the first stage , we evaluate from training data the relevance of relation path Pathl = [ Ai , Rel1,?, Reln , Aj ] between candidate anchors Ai and Aj of types Ci and Cj . We divide this task into 2 steps.
The first step ranks each single relation Relk ? Pathl ; while the second step combines the evaluations of Relk to rank the whole relation path Pathl.
Single relation ranking
Let Seti and Setj be the set of linguistic features of anchors Ai and Aj respectively . To evaluate Relk , we consider 2 characteristics : (1) the direction of relation Relk as encoded in the tree structure ; and (2) the linguistic features , Seti and Setj , of anchors Ai and Aj . We need to construct multiple single relation classifiers , one for each anchor pair of types Ci and Cj , to evaluate the relevance of Relk with respect to these 2 anchor typ
Preprocessing Corpus ( a ) Construction of classifiers . The training data to each classifier consists of anchor pairs of types Ci and Cj extracted from the training corpus . We use these anchor pairs to construct each classifier in four stages . First , we compose the set of possible patterns in the form P + = { Pm = < Si ? Rel -> Sj > | Si ? Seti , Sj ? Setj }. The construction of Pm
Anchor evaluation
Templates
Anchor NEs
Template evaluation
Sentences
Binary relationship evaluation
Candidate templates
Figure 4 illustrates several discourse and dependency patterns of P + constructed from a sample sentence.

Figure 4. Examples of discourse and dependency patterns Second , we identify the candidate anchor A , whose type matches slot C in a template . Third , we find the correct patterns for the following 2 cases : 1) Ai , Aj are of correct anchor types ; and 2) Ai is an action anchor , while Aj is a correct anchor . Any other patterns are considered as incorrect . We note that the discourse and dependency paths between anchors Ai and Aj are either correct or wrong simultaneously.

Fourth , we evaluate the relevance of each pattern Pm ? P +. Given the training set , let PairSetm be the set of anchor pairs extracted by Pm ; and PairSet+(Ci , Cj ) be the set of correct anchor pairs of types Ci , Cj . We evaluate both precision and recall of Pm as |||| |),(|| )( m jim m PairSet
CCPairsSetPairSet
PrecisionP |= +? (3) ||),(|| |),(|| )( ji jim m CCPairsSet
CCPairsSetPairSet
PecallR + + |= ? (4)
These values are stored and used in the training model for use during testing.
( b ) Evaluation of relation . Here we want to evaluate whether relation InputRel belongs to a path between anchors InputAi and InputAj . We employ the constructed classifier for the anchor types InputCi and InputCj in 2 stages . First , we find a subset P(0) = { Pm = < Si ? InputRel -> Sj > ? P + | Si ? InputSeti , Sj ? InputSetj } of applicable patterns . Second , we utilize P(0) to find the pattern
Pm(0) with maximal precision : Precision(Pm (0)) = argmaxPm?P(0) Precision ( Pm ) (5) A problem arises if Pm(0) is evaluated only on a small amount of training instances . For example , we noticed that patterns that cover 1 or 2 instances may lead to Precision=1, whereas on the testing corpus their accuracy becomes less than 50%.
Therefore , it is important to additionally consider the recall parameter of Pm(0).
Relation path ranking
In this section , we want to evaluate relation path connecting template slots Ci and Cj . We do this independently for each relation of type discourse and dependency . Let Recallk and Precisionk be the recall and precision values of Relk in Path = [ Ai , Rel1,?, Reln , Aj ], both obtained from the previous step . First , we calculate the average recall of the involved relations : W = (1/LengthPath ) * ? Relk?Path Recallk (6) W gives the average recall of the involved relations and can be used as a measure of reliability of the relation Path . Next , we compute a combined score of average Precisionk weighted by Recallk : Score = 1/(W*LengthPath)*?Relk?Path Recallk*Precisionk (7) We use all Precisionk values in the path here , because omitting a single relation may turn a correct path into the wrong one , or vice versa . The combined score value is used as a ranking of the relation path . Experiments show that we need to give priority to scores with higher reliability W . Hence we use ( W , Score ) to evaluate each Path.
5.2 Integration of different relation path types The purpose of this stage is to integrate the evaluations for different types of relation paths . The input to this stage consists of evaluated relation paths PathC and PathD for discourse and dependency relations respectively . Let ( Wl , Scorel ) be an evaluation for Pathl , l ? [ c , d ]. We first define an integral path PathI between Ai and Aj as : 1) PathI is enabled if at least one of Pathl , l ? [ c , d ], is enabled ; and 2) PathI is correct if at least one of Pathl is correct . To evaluate PathI , we consider the average recall Wl of each Pathl , because Wl esti-elaboration obj
Anchor Aj town pos_NN
Cand_AtArg2
Minipar_pcompn
ArgM-Loc
Spade_Satellite
Anchor Ai
Marshal pos_NNP list_personWord
Cand_AtArg1
Minipar_obj
Arg2
Spade Satellite pcompn fc span
Discourse path
Dependency path i elaboration
Input sentence
Marshal ? named < At-A1> </> played by Amourie Kats who discovers all kinds of on and scary things going on in < At-A2>
Dependency patterns
Minipar_obj <? i - ArgM-Loc
Minipar_obj <? obj - ArgM-Loc
Minipar_obj ? pcompn -> Minipar_pcompn
Minipar_obj ? mod -> Minipar_pcompn ? a seemingly quiet little town </> ...
elaboration pnmod pred i null null rel i mod
Discourse patterns list_personWord <? elaboration - pos_NN list_personWord ? elaboration -> town list_personWord <? span - town list_personWord <? elaboration - town ? weighted average for Pathl as : WI = WC + WD (8) ScoreI = 1/WI * ? l Wl*Scorel (9) Next , we want to determine the threshold score ScoreIO above which ScoreI is acceptable . This score may be found by analyzing the integral paths on the training corpus . Let SI = { PathI } be the set of integral paths between anchors Ai and Aj on the training set . Among the paths in SI , we need to define a set function SI(X ) = { PathI | ScoreI(PathI ) ? X } and find the optimal threshold for X . We find the optimal threshold based on F1-measure , because precision and recall are equally important in IE . Let SI(X )+ ? SI(X ) and S(X )+ ? S(X ) be sets of correct path extractions . Let FI(X ) be F1-measure of SI(X ): ||)(|| ||)(|| )(
XS
XS
XP
I
I
I + = (10) ||)(|| ||)(|| )( + + =
XS
XS
XR II (11) )()( )(*)(*2 )(
XRXP
XRXP
XF
II
II
I += (12)
Based on the computed values FI(X ) for each X on the training data , we determine the optimal threshold as Score = argmax F ( X)IO X I , which corresponds to the maximal expected F1-measure of anchor pair Ai and Aj.
5.3 Evaluation of templates
At this stage , we have a set of accepted integral relation paths between any anchor pair Ai and Aj.
The next task is to merge appropriate set of anchors into candidate templates . Here we follow the methodology of Maslennikov et al (2006). For each sentence , we compose a set of candidate templates T using the extracted relation paths between each Ai and Aj . To evaluate each template Ti?T , we combine the integral scores from relation paths between its anchors Ai and Aj into the overall Relation_ScoreT:
M
AAScore
TScoreelationR Kji jiI iT ? ??= ,1 ),()(_ (13) where K is the number of extracted slots , M is the number of extracted relation paths between anchors Ai and Aj , and ScoreI(Ai , Aj ) is obtained from Equation (9).
Next , we calculate the extracted entity score based on the scores of all the anchors in Ti : ? ??= Kk kiT KAScoreEntityTScoreEntity 1 /)(_)(_ (14) where Entity_Score(Ai ) is taken from Equation (1).
Finally , we obtain the combined evaluation for a template : ScoreT(Ti ) = (1- ?) * Entity_ScoreT ( Ti ) + ? * Relation_ScoreT ( Ti ) (15) where ? is a predefined constant.
In order to decide whether the template Ti should be accepted or rejected , we need to determine a threshold ScoreTO from the training data . If anchors of a candidate template match slots in a correct template , we consider the candidate template as correct . Let TrainT = { Ti } be the set of candidate templates extracted from the training data , TrainT + ? TrainT be the subset of correct candidate templates , and TotalT + be the total set of correct templates in the training data . Also , let TrainT(X ) = { Ti | ScoreT(Ti ) ? X , Ti ? TrainT } be the set of candidate templates with score above X and TrainT+(X ) ? TrainT(X ) be the subset of correct candidate templates . We define the measures of precision , recall and F1 as follows : ||)(|| ||)(|| )(
XTrainT
XTrainT
XPT + = (16) |||| ||)(|| )( + + =
TotalT
XTrainT
XRT (17) )()( )()(*2 )(
XRXP
XRXP
XF
TT
TT
T += (18)
Since the performance in IE is measured in F1-measure , an appropriate threshold to be used for the most prominent candidate templates is : ScoreT
O = argmaxX FT ( X ) (19)
The value ScoreTO is used as a training model.
During testing , we accept a candidate template In-putTi if ScoreT(InputTi ) > Sco Ore . T As an additional remark , we note that domains MUC4, MUC6 and ACE RDC 2003 are significantly different in the evaluation methodology for the candidate templates . While the performance of the MUC4 domain is measured for each slot individually ; the MUC6 task measures the performance on the extracted templates ; and the ACE RDC 2003 task evaluates performance on the matching relations . To overcome these differences , we construct candidate templates for all the domains and measure the required type of performance for each domain . Our candidate templates for the ACE RDC 2003 task consist of only 2 slots , which correspond to entities of the correct relations.
597 6 Experimental results
We carry out our experiments on 3 domains : MUC4 ( Terrorism ), MUC6 ( Management Succession ), and ACE-Relation Detection and Characterization (2003). The MUC4 corpus contains 1,300 documents as training set and 200 documents ( TST3 and TST4) as official testing set . We used a modified version of the MUC6 corpus described by Soderland (1999). This version includes 599 documents as training set and 100 documents as testing set . Following the methodology of Zhang et al . (2006), we use only the English portion of ACE RDC 2003 training data . We used 97 documents for testing and the remaining 155 documents for training . Our task is to extract 5 major relation types and 24 subtypes.

Case (%) P R F1
GRID 52% 62% 57%
Riloff?05 46% 51% 48%
ARE (2006) 58% 61% 60%
ARE 65% 61% 63%
Table 2. Results on MUC4
To compare the results on the terrorism domain in MUC4, we choose the recent state-of-art systems GRID by Xiao et al (2004), Riloff et al (2005) and ARE (2006) by Maslennikov et al (2006) which does not utilize discourse and semantic relations . The comparative results are given in Table 2. It shows that our enhanced ARE results in 3% improvement in F1 measure over ARE (2006) that does not use clausal relations . The improvement was due to the use of discourse relations on long paths , such as ? X distributed leaflets claiming responsibility for murder of Y ?. At the same time , for many instances , it would be useful to store the extracted anchors for another round of learning.
For example , the extracted features of discourse pattern ? murder ? same_clause -> HUM_PERSON ? may boost the score for patterns that correspond to relation path ? X <- span - _ - Elaboration -> murder ?. In this way , highprecision patterns will support the refinement of patterns with average recall and low precision . This observation is similar to that described in Ciravegna?s work on ( LP)2 ( Ciravegna 2001).

Case (%) P R F1
Chieu et al?02 75% 49% 59%
ARE (2006) 73% 58% 65%
ARE 73% 70% 72%
Table 3. Results on MUC6
Next , we present the performance of our system on MUC6 corpus ( Management Succession ) as shown in Table 3. The improvement of 7% in F1 is mainly due to the filtering of irrelevant dependency relations . Additionally , we noticed that 22% of testing sentences contain 2 answer templates , and entities in many of such templates are intertwined . One example is the sentence ? Mr . Bronczek who is 39 years old succeeds Kenneth Newell 55 who was named to the new post of senior vice president ?, which refers to 2 positions . We therefore we need to extract 2 templates ? PersonIn : Bronczek , PersonOut : Newell ? and ? PersonIn : Newell , Post : senior vice president ?. The discourse analysis is useful to extract the second template , while rejecting another long-distance template ? PersonIn : Bronczek , PersonOut : Newell , Post : seniour vice president ?. Another remark is that it is important to assign 2 anchors of ? Cand_PersonIn ? and ? Cand_PersonOut ? for the phrase ? Kenneth Newell?.
The characteristic of the ACE corpus is that it contains a large amount of variations , while only 2% of possible dependency paths are correct . Since many of the relations occur only at the level of single clause ( for example , most instances of relation At ), the discourse analysis is used to eliminate long-distance dependency paths . It allows us to significantly decrease the dimensionality of the problem . We noticed that 38% of relation paths in ACE contain a single relation , 28% contain 2 relations and 34% contain ? 3 relations . For the case of ? 3 relations , the analysis of dependency paths alone is not sufficient to eliminate the unreliable paths . Our results for general types and specific subtypes are presented in Tables 6 and 7, respectively.

Case (%) P R F1
Zhang et al?06 77% 65% 70%
ARE 79% 66% 73%
Table 4. Results on ACE RDC?03, general types Based on our results in Table 4, discourse and dependency relations support each other in different situations . We also notice that multiple instances require modeling of entities in the path.
Thus , in our future work we need to enrich the search space for relation patterns . This observation corresponds to that reported in Zhang et al (2006).
Discourse parsing is very important to reduce the amount of variations for specific types on ACE
Case (%) P R F1
Zhang et al?06 64% 51% 57%
ARE 67% 54% 61%
Table 5. Results on ACE RDC?03, specific types The relatively small improvement of results in Table 5 may be attributed to the following reasons : 1) it is important to model the commonality relations , as was done by Zhou et al (2006); and 2) our relation paths do not encode entities . This is different from Zhang et al (2006), who were using entities in their subtrees.
Overall , the results indicate that the use of discourse relations leads to improvement over the state-of-art systems.
7 Conclusion
We presented a framework that permits the integration of discourse relations with dependency relations . Different from previous works , we tried to use the information about sentence structure based on discourse analysis . Consequently , our system improves the performance in comparison with the state-of-art IE systems . Another advantage of our approach is in using domain-independent parsers and features . Therefore , ARE may be easily portable into new domains.
Currently , we explored only 2 types of relation paths : dependency and discourse . For future research , we plan to integrate more relations in our multiresolution framework.
References
P . Cimiano and U . Reyle . 2003. Ontology-based semantic construction , underspecification and disambiguation . In Proc of the Prospects and Advances in the Syntax-
Semantic Interface Workshop.
P . Cimiano , U . Reyle and J . Saric . 2005. Ontology-driven discourse analysis for information extraction . Data &
Knowledge Engineering , 55(1):59-83.
H.L . Chieu and H.T . Ng . 2002. A Maximum Entropy Approach to Information Extraction from Semi-Structured and Free Text . In Proc of AAAI-2002.
F . Ciravegna . 2001. Adaptive Information Extraction from Text by Rule Induction and Generalization . In Proc of
IJCAI-2001.
A . Culotta and J . Sorensen J . 2004. Dependency tree kernels for relation extraction . In Proc of ACL2004.
A . Dempster , N . Laird , and D . Rubin . 1977. Maximum likelihood from incomplete data via the EM algorithm.
Journal of the Royal Statistical Society B , 39(1):1?38.
B . Grosz and C . Sidner . 1986. Attention , Intentions and the Structure of Discourse . Computational Linguistics , 12(3):175-204.
M . Halliday and R . Hasan . 1976. Cohesion in English.
Longman , London.
D . Lin . 1997. Dependency-based Evaluation of Minipar . In Workshop on the Evaluation of Parsing systems.
M . Maslennikov , H.K . Goh and T.S . Chua . 2006. ARE : Instance Splitting Strategies for Dependency Relation-based Information Extraction . In Proc of ACL2006.
E . Miltsakaki . 2003. The Syntax-Discourse Interface : Effects of the Main-Subordinate Distinction on Attention
Structure . PhD thesis.
M.F . Moens and R . De Busser . 2002. First steps in building a model for the retrieval of court decisions . International Journal of HumanComputer Studies , 57(5):429-446.
S . Pradhan , W . Ward , K . Hacioglu , J . Martin and D . Jurafsky . 2004. Shallow Semantic Parsing using Support Vector Machines . In Proc of HLT/NAACL-2004.
E . Riloff , J . Wiebe , and W . Phillips . 2005. Exploiting Subjectivity Classification to Improve Information Extraction . In Proc of AAAI-2005.
S . Soderland . 1999. Learning Information Extraction Rules for Semi-Structured and Free Text . Machine Learning , 34:233-272.
R . Soricut and D . Marcu . 2003. Sentence Level Discourse Parsing using Syntactic and Lexical Information . In
Proc of HLT/NAACL.
M . Surdeanu , S . Harabagiu , J . Williams , P . Aarseth . 2003.
Using Predicate Arguments Structures for Information
Extraction . In Proc of ACL2003.
M . Taboada and W . Mann . 2005. Applications of Rhetorical Structure Theory . Discourse studies , 8(4).
B . Webber , M . Stone , A . Joshi and A . Knott . 2002.
Anaphora and Discourse Structure . Computational Linguistics , 29(4).
J . Xiao , T.S . Chua and H . Cui . 2004. Cascading Use of Soft and Hard Matching Pattern Rules for Weakly Supervised Information Extraction . In Proc of COLING2004.
M . Zhang , J . Zhang , J . Su and G . Zhou . 2006. A Composite Kernel to Extract Relations between Entities with both Flat and Structured Features . In Proc of ACL2006.
G . Zhou , J . Su and M . Zhang . 2006. Modeling Commonality among Related Classes in Relation Extraction . In
Proc of ACL2006.
599
