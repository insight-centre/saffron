FORUMONCONNECTIONISM
Connectionist Models for Natural Language Processing 
David L . Waltz
Thinking Machines Corporation
245 First Street
Cambridge , MA 02142

Program in Linguistics and Cognitive Science
Brandeis University
Brown 125
Waltham , MA 02254
PANELIST STATEMENT
After an almost twenty year lull , there has been a dramatic upsurge of interest in massively parallel models for computation  , descendants of perceptron and pan demonium models , now dubbed ' connectionist models . ' Much of the connectionist research has focused on models for natural an-guage processing  . There have been three main reasons for this increase in interest :  1  . Scientific adequacy of the models 2 . The availability of finegrained parallel hardware to run the models  3  . The demonstration of powerful connectionist learning models  . 
The scientific adequacy of models based on a small number of coarse-grained primitives  ( e . g . conceptual dependency ) , popular in AI during the 70's , has been called into question and substantially replaced by a current emphasis in much of computational linguistics on lexicalist models  ( i . e . , ones which use words for representing concepts or meanings  )  . However , few people can doubt that words are too coarse , that they have structure and properties and features  . Connectionist models offer very fine granularity ; they can capture such detail in a manner that still allows for tractable computation  . 
Such models also promise to make the integration of syntactic  , semantic , pragmatic , and memory models simpler and more transparent . 
Fine-grained hardware , such as the Connection Machine , can allow models with millions of active elements  , full vocabularies , and rapid through put , as well as powerful near-term connectionist applications based on the use of associative memory and hardware support for interprocessor communication  . Meanwhile , connectionist learning models , such as the Boltzmann Machine and its descendant , he backward error propagation model , have demonstrated surprising power in learning concepts from example  ; as for instance in Sejnowski's NET talk , which learned the pronunciation rules for English from examples  . The future promises yet more surprising results as the concepts in even more radical models  , such as Minsky's Society of Minds model , are digested and as new , even more powerful hardware becomes available . 

