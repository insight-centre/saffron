Improved Iterative Scaling can yield multiple globally optimal 
models with radically diering performance levels
Iain Bancarz and Miles Osborne

Division of Informatics
University of Edinburgh
2 Buccleuch Place
Edinburgh EH8 9LW


Loglinear models can be e?ciently estimated using algorithms such as Improved Iterative Scaling  ( IIS ) (La erty et al ,  1997) . Under certain conditions and for a particular class of problems  , IIS is guaranteed to approach both the maximum -likelihood and maximum entropy solution  . This solution , in likelihood space , is unique . Unfortunately , in realistic situations , multiple solutions may exist , all of which are equivalent to each other in terms of likelihood  , but radically dierent from each other in terms of performance  . We show that this behaviour can occur when a model contains overlapping features and the training material is sparse  . Experimental results , from the domain of parse selection for stochastic attribute value grammars  , shows the wide variation in performance that can be found when estimating models using IIS  . Further results show that the in-uence of the initial model can be diminished by selecting either uniform weights  , or else by model averaging . 
1 Background
When statistically modelling linguistic phenomena of one sort or another  , researchers typically tloglinear models to the data  ( for example ( Johnson et al . , 1999)) . There are ( at least ) three reasons for the popularity of such models : they do not make un-warranted independence assumptions  , the maximum likelihood solution of such models coincides with the maximum entropy solution  , and nally , they can be e?ciently estimated ( using algorithms such as Improved Iterative Scaling  ( IIS )   ( Laerty et al ,  1997)) . 
Now , the solution found by IIS is guaranteed to approach a global maximum for both likelihood and entropy under certain conditions  . Although this is appealing , in realistic situations it turns out that multiple models exist  , all of which are equivalent in terms of likelihood but dierent from each other in terms of their performance at some task  . In particular , the initial weight settings caninuence the quality of the nal model  , even though this nal model is the maximum entropy solution  ( as found by IIS )  . 
A tr st glance , this seems very strange . The IIS algorithm is guaranteed to converge to a globally optimal solution regardless of the initial parameters  . 
If the initial weights assigned to some of the features are wildly inappropriate then the algorithm may take longer to converge  , but one would expect the naldestination to remain the same  . However , as we show later , what is unique in terms of likelihood need not be unique in terms of performance  , and so IIS can be sensitive to the initial weight settings  . 
Some of the reason for this behaviour may lie in a relatively subtle eect which we call overlapping features  . 

If some features behave identically in the training set  ( but not necessarily in future samples )  , IIS cannot distinguish between dierent sets of weights for those features unless the sum of all weights in each set is dierent  . In fact , then alweights assigned to such features will be dependent on their initial values  . Under these conditions , there will be a family of models , all of which are identical as far as IIS is concerned  , but distinguishable from each other in terms of performance  . This means that in terms of performance space , the landscape will contain local maxima . In terms of likelihood space , the landscape will continue to contain just the single  ( global ) maximum . 
This indeterminacy is clearly undesirable . However , there are ( at least ) two practical ways of dealing with it : one either initialises IIS with weights that are identical  ( 0 is a good choice )  , or else one takes a Bayesian approach , and averages over the space of models . In this paper , we show that the performance of IIS is sensitive to the initial weight settings  . We show that setting the weights to zero yields performance that is better than a large set of randomly initialised models  . Also , we show that model averaging can also reduce in determinacies introduced through the choice of initial weights  . 
The rest of this paper is as follows . Section 2 is a restatement of the theory behind IIS . Section 3 shows how sparse training material , coupled reason for our observe deects . As an anonymous reviewer noted , sparse statistics may also yield similarndings . 
with large models can result in situations whereby IIS produces suboptimal results  . We then move on to experimental support for our theoretical results  . Our domain is parse selection for broadcoverage stochastic attribute-value grammars  . Section 4 shows how we model the broadcoverage attribute -value grammar  ( mentioned in section 5 used in our experiments )  , and then present our results . Therst set of experiments ( section 7 ) deals with how well IIS , with uniform initial settings . outperforms models with randomised initial settings  . 
The second set of experiments shows how model averging can deal with the problem of initialising the model  . We conclude the paper with some comments on our work  . 
2 The Duality Lemma
How can we be certain that IIS seeks out a global maximum for both likelihood and entropy ? The answer is that for the class of problems under consideration  , there is only a single maximum-which is then necessarily the global one  . The IIS algorithm simply ` hill-climbs ' and seeks to increase the likelihood of the model being trained  . Its success is guaranteed because of a result which we refer to as the Duality 

The proof of the Duality Lemma is contained in ( Laerty et al , 1997) but will be omitted here . In order to state the lemma , werst dene the setting and establish some notation  . 
Suppose that we have a probability measure space (   ; F ; P ) , whereas usual is a set made up of elements ! , F is a sigma-eld on , and P is a probability measure on F . Suppose further that X is a simple random variable on-that is to say  , it is a real-valued function on , having niterange , and such that [!: X(!)=x]2F . As usual , we will omit the argument ! , so that X indicates a general value of the function as well as the function itself  , and x denotes [!: X(!)=x] . We will also abuse notation by letting X denote the set of possible values of x  ; the meaning should be clear from context . 
We now consider a stochastic process on ( ; F ; P ) . 
We are given a sample of past outputs ( data points ) from this process which make up the training data . 
We again abuse notation by using ~ p to refer to both the set of data points and the distribution dened by that set  . Let  denote the set of all possible probability distributions over X  ; we seek a model q  2  which is in some sense the best possible probability distribution over future outputs  . We specically examine generalized Gibbs distributions  , which are of the form : q h ( x ) = q ( h ) ) exph ( x ) q ( x )   ( 1 ) In this case , h is a real-valued function on X , q is an initial probability distribution over X ( which may be the uniform distribution )  , and Zq(h ) is a normalising constant , taking a value such that
P x2 X q h(x ) = 1.
The function hhere takes the form : h(x ) = n
X i=1  ifi ( x )  =  (  f ) ( x )   ( 2 ) where the f i ( x ) are integer feature functions . The real numbers  i are adjustable parameters . 
Suppose that we are given the data ~ p , an initial model q ( 1997 ) describe two natural sets of models . Therst is the set P ( f ; ~ p ) of all distributions that agree with ~ pasto the expected value of the feature function f : P  ( f ; ~ p ) = [ p2:p[f ] = ~ p[f ]] (3) where , as usual , p[f ] denotes the expectation of the function f under the distribution p  . 
The second is Q(f ; q
Gibbs distributions based on qf :
Q(f ; q
Let Q denote the closure of Q in  , with respect to the topology  inherits as a subset of Euclidean space  . 
These in turn determine two natural candidates for the ` best'model q   . Let D ( pkq ) denote the KullbackLeibler divergence between two distributions p and q  . The suitable models are :  Maximum Likelihood Gibbs Distribution  . A distribution in Q with maximum likelihood with respect to ~ p : q 
ML  = argmin q2Q

 Maximum Entropy Constrained Distribution . A distribution in P with maximum entropy relative to q 
ME  = argmin q2P ( f ; ~ p)

The key result of Laerty et al ( 1997 ) is that there is a unique qsatisfying q=q
ML=q
ME  .
In Appendix 1 of that paper , the following result is proved :
The Duality Lemma . Suppose that D(~pkq1 . Then there exists a unique q2 satisfying : 1 . q2P\Q2 . D(pkq ) = D(pkq)+D(qkq ) 8 p2P;q2Q3 . q  = argmin q2Q
D(~pkq ) 4. q=argmin q2P ( f ; ~ p)

The Duality Lemma is a very useful result , fundamental to the ability of IIS to converge upon the single maximum likelihood  , maximum entropy solution . 
The IIS algorithm itself uses a fairly straightforward technique to look for any maximum of the likelihood function  ; because of the lemma , IIS is guaranteed to approach the solution q . 
3 Limitations With Sparse Training
Data 3.1 Overlapping Features
In this paper , all models have the same training data and a uniform initial distribution q that D  ( pkq algorithm approaches the same optimal distribution q   . However , our experiments show that not all distributions obtained by running IIS  ( to convergence ) are equally good at modelling a set of test data . Performance appears to depend ( at least ) on the starting values for the weights  i . This may be a result of the following situation :
Consider two features , fi and fj with i < j , with weights  i and  j respectively . Suppose that fi ( x ) = fj ( x ) for all values of x in ~ p , but there exist values of x outside ~ p such that f i  ( x ) 6= fj ( x )  ; that is , the two functions take exactly the same values on the set of training data but dier outside of it  . 
This phenomenon can be called overlapping.

Overlapping features are commonly found in maximum entropy models  , and are one of the main reasons for their popularity  . In fact , one could argue that all maximum entropy models found in natural language applications contain overlapping features  . Overlapping features may be present by explicit design  ( for example whenemulating backing-o smoothing )  , or else naturally , for example when using features to treat words cooccurring with each other  . 
We assign initial weights  ( 0 ) i and  ( 0 ) j to the features , and after n iterations of the algorithm , they have been adjusted to  ( n ) i and  ( n ) j respectively . 
Now consider the target solution q , as determined by q  is of the form : q= exp  ( n
X k=1 kfk )   ( 5 ) where Z is the usual normalising constant . Notice that qmay not belong to the family of exponential models Q  , but instead may be part of the larger set Q . Indeed , della Pietra et al state that P\Q may be empty . This is not a serious limitation as one can come arbitrarily close to any element of 
Q while remaining inside Q . Thus , if q=2Q , we may consider the above expression to be a very close approximation to q   , such as could be obtained by running IIS to convergence  . 
Because the ith and jth features are equal on the training data  , the exact values of  i and  j have no eect on the likelihood of the model as long a stures to largely cooccur together  . Depending upon the degree of cooccurrence , we would expect to continuet on dour results . 
their sum remains the same . In this instance , q  is not a single model at all , but rather a family of models satisfying the condition  i + j=ij for a particular  ij  . 

All models in this family assign equal likelihood to the training data  , and so the IIS algorithm is unable to distinguish between them  . 
Under these circumstances we should ensure that   i =  j  . Since we have no way to distinguish between the two features  , our model should assign them equal importance . However , this is not guaranteed by the IIS algorithm . In particular , it is less likely to occur if the initial weights     ( 0 ) i and  ( 0 ) j are not equal . 
3.2 IIS and Overlapping Features-A
Simple Example
Suppose that our model has a vector of parameters  = [ i : i =  1 ::: n ] and we wish to change it to +? , where ? = [ ? i : i = 1 ::: n]TheIIS algorithm considers each  i in turn  . It chooses ? i to maximize an auxiliary function B  ( ? j )  , which provides a lower bound on the change in loglikelihood of the model  . 
Each adjustment  i  i + ? i is guaranteed to increase the likelihood of the model and thus approach our ideal solution q  . This process is repeated until convergence . There are no inherent restrictions on the value of ? i  ; it may be positive or negative , and large or small compared to the values taken for dierent features  . 
Now , suppose that the features fi and fj overlap and we halt the algorithm after titerations  . 
Clearly , the algorithm cannot guarantee that the-nal weights  ti and  tj will be equal  . The following highly simplied example should demonstrate why this is the case  . 
Example 1: Imagine that our model has two overlapping features f  is the family of models in which  the initial weights are   ( 0 )  = 0 . Recall that at the ith step of an iteration , the IIS algorithm considers the change in loglikelihood of the model which can be made by only adjusting the ith parameter  . In this case no change is possible as the sum  algorithm terminates with  t t have assigned far greater importance to ff assumption might not be warranted  . 
3.3 IIS and Overlapping Features in

The situation will obviously be much more complicated in practice  . Most importantly , there is no such ij is not a ` constant ' as such  , since any vector of weights is in a sense unique only up to multiplication by a positive constant  . This will be examined in greater detail when we consider overlapping features in practice  . 
thing as an absolute \ optimum " vector of weights      . As a result , no one parameter can be regarded as xed until the algorithm has converged for all parameters  . In the above example , our \ true " set of target solutions is those for which  stant  . Since there are no other features , IIS would in fact terminate at once for any pair of initial weights  . 
Suppose that we added a third feature f did not overlapt herst two  . Our set of target solutions would then be of the form  
The adjustments made to  those made to  sarily terminate after the rst pass  . 
The following example will describe what happens in this more realistic situation  . It will also explain the possible benets of setting all initial weights to the same value  . 
Example 2: Let the model have the three features described above  , with initial weights =5 ;   = 0 ;   =  1 and a family of target solutions qdened by  again terminates at once  , because the initial model is already part of the family q   . Again there is an unjustied dierence between then alweights for fNow suppose that all three initial weights are set to zero  . Imagine for simplicity that we have restricted the algorithm to adjust weights only by zero or  1  . On the rst pass , all three weights are changed to 1 . On the second ,  to 2;  to 3,  terminates . 

Notice that , although there is still a dierence between the nal weights for f than before  . This more closely approaches the ideal situation in which the weights are equal  . 
This concludes our theoretical treatment of IIS.
We now show experimentally the inuence that the initial weight settings have  , and how it can be minimised . Our strategy is to use plausible features , as found in a realistic domain ( parse selection for stochastic attribute-value grammars  )  , and rstly show what happens when the initial weight settings are set uniformly  ( to zero )  . We then show what happens when these initial settings are randomly set  , and nally , what happens when we average over randomly initialised maximum likelihood solutions  . 

The exact behaviour of the algorithm will depend on the training data  , so this is not the only imaginable outcome , but it is certainly a plausible one . 
4 Loglinear Modelling of
Attribute Value Grammars
Here we show how attribute-value grammars may be modelled using loglinear models  . Abney gives fuller details ( Abney , 1997) . 
Let G be an attribute-value grammar , and D a set of sentences within the string-set de ned by L  ( G )  . 
A loglinear model , M , consist of two components : a set of features , F and a set of weights ,   . 
The ( unnormalised ) total weight of a parse x , ( x ) , is a function of the k features that are ` active ' on a parse:  ( x ) = exp ( k
X i=1  ifi(x )) (6)
The probability of a parse , P ( xjM ) , is simply the result of normalising the total weight associated with that parse : 
P ( xjM ) = ( x ) (7)
Z =
X y2 ( y )   ( 8 ) is the union of the set of parses assigned to each sentence in D by the grammar G  , such that each parse in is unique in terms of the features that are active on it  . Normally a parse can be viewed as the set of features that are active on it  . 
The interpretation of this probability ( equation 7 ) depends upon the application of the model . Here , we use parse probabilities to reect preferences for parses  . 
5 The Grammar
The grammar we model with loglinear models ( called the Tag Sequence Grammar ( Briscoe and Carroll ,  1996) , or TSG for short ) was manually developed with regard to coverage , and when compiled consists of 455 Denite Clause Grammar ( DCG ) rules . It does not parse sequences of words directly , but instead assigns derivations to sequences of part-of-speech tags  ( using the CLAWS2 tagset )  . The grammar is relatively shallow ( for example , it does not fully analyse unbounded dependencies ) . 
6 Modelling the Grammar
Modelling the TSG with respect to the parsed Wall Street Journal consists of two steps : creation of a feature set and denition of a reference distribution  ( the target model , ~ p ) . 
6.1 Feature Set
Modelling the TSG with respect to the parsed Wall Street Journal consists of two steps : creation of a 

A1/app1: unimpededaaa!!!unimpededPP/p1:by
P1/pn1:bybb""byN1/n:tra?ctra?c
Figure 1: TSG Parse Fragment feature set and denition of the reference distribution  . 
Our feature set is created by parsing sentences in the training set  , and using each parse to instantiate templates . Each template denes a family of features . Our templates are motivated by the observations that linguistically-stipulated units  ( DCG rules ) are informative , and that many DCG applications in preferred parses can be predicted using lexical information  . 
Therst template creates features that count the number of times a DCG instantiation is present within a parse  . 

For example , suppose we parsed the Wall Street Journal AP : 1 unimpeded by tra?cA parse tree generated by TSG might be as shown in gure  1  . Here , to save on space , we have labelled each interior node in the parse tree with TSG rule names  , and not attribute-value bundles . Furthermore , we have annotated each node with the head word of the phrase in question  . Within our grammar , heads are ( usually ) explicitly marked . This means we do not have to make any guesses when identifying the head of a local tree  . With head information , we are able to lexicalise models . We have suppressed tagging information . 
For example , a feature dened using this template might count the number of times the we saw: 

A1/app1 in a parse . Such features record some of the context of the rule application  , in that rule applications that in a local tree . Lexical information is included when we decide to lexicalise features  . 
dier in terms of how attributes are bound will be modelled by dierent features  . 
Our second template creates features that are partially lexicalised  . For each local tree ( of depth one ) that has a PP daughter , we create a feature that counts the number of times that local tree  , decorated with the head word of the PP , was seen in a parse . 
An example of such a lexicalised feature would be :


These features are designed to model PP attachments that can be resolved using the head of the 

The third and naltemplate creates features that are again partially lexicalised  . This time , we create local trees of depth one that are decorated with the head word  . For example , here is one such feature :


Note the second and third templates result in features that overlap with features resulting from applications of the rst template  . 
6.2 Reference Distribution
We create the reference distribution R ( an association of probabilities with TSG parses of sentences  , such that the probabilities reect parse preferences  ) using the following process : 1 . Take the training set of parses and for each parse  , compare the structural dierences between it and a reference treebank parse  . 
2 . Map these tree similarity scores into probabilities  ( where the sum of all reference probabilities for all parses sums to one  )  . 
Again , see Anon for more details.
This concludes our discussion of how we model grammars  . We now go on to present our experimental investigation of the inuence of the initial weight settings  . 
7 Experiments
Here we present three sets of experiments . Therst set shows the performance of maximum entropy when the initial weight setting are zero  . The second sets how the eects of randomised initial setting  , and so establishes ( an estimate of ) the variation in performance space . The third set of experiments showed how the in uence of the initial weight settings could be minimised by averaging over many h models  . 
Throughout , we used the same training set . This consisted of a sample of 53795 parses ( produced from sentences at most 15 tokens long , with at most 15 parses per sentence ) . The sentences were drawn from the parsed Wall Street Journal  , and all could be parsed using our grammar . The motivation for this choice of training set came from the fact that when the sample of sentences is too small  , the resulting model will tend to undert . Likewise , when the training set is too large , the model will tend to over-t . A sample of appropriate size ( which can be found using a simple search , as Osborne ( 2000 ) demonstrated ) will therefore neither signicantly undertnor over t  . Quite apart from estimation issues related to sample size  , because we repeatedly estimate models , using a sample that is just su?ciently large ( and no larger ) allows us to make signicant computational savings  . 
We used a disjoint development set and testing set  . The development set consisted of 2620 parses , derived from parsing sentences at most 30 tokens long , with at most 100 parses per sentence . The testing set was randomly sampled from the Wall Street Journal  , and consisted of 469 sentences , with each sentence at most 30 tokens long , with at most 100 parses per sentence . Each sentence has on average 60:0 parses per sentence . 
The model we used throughout contained 75171 features . 
For all experiments , we ran IIS for the same number of iterations (75) . This number was selected as the number of iterations that produced the best performance for maximum entropy  ( our yard stick )  . 
Evaluation was in terms of exact match : for each sentence in the test set  , we awarded ourselves a point if the estimated model ranked highest the same parse that was ranked highest using the reference probabilities  . 

Ties were randomly broken.
7.1 Maximum Entropy Results using
Uniform Initialisation
A model trained using weights all initialised to zero yielded an exact match score of  52:0%  . 
7.2 Randomised Models
A pool of 10 ;   000 models was created by randomly setting the initial weights to values in the range  [-0  . 3,0 . 3] and then estimating the nal weights using the training set  . 
The histogramingure 2 shows the number of models that produced the same results on the testing material  . As can be seen , performance is roughly normally distributed , with some local minima having a much wider basin of attraction than other minima  . Also , note that all of these models underper-The particular choice of metric is not crucial to the results of this paper  . 
forms a model crated using uniform initialisation.

The performance of the worst model was 24:1%.
This is our lower bound , and is less than half that of basic maxent . The best randomly selected model had a performance of  46:5%  . 

Number of Models
ExactMatch " rand-hist 04"
Figure 2: Distribution of models with randomly initialised starting conditions  7  . 3 Averaging over models To see whether an ensemble of such randomised models could cancel-out the in uence of the initial weight settings  , we created a pool of 600 randomised models , and then combined then together using equation 9:
P ( xjMn ) =
Qn i=1
P ( xj  Mi )
Py
Qn j=1
P ( yjMj ) (9)
Because it is possible that some subset of the models outperforms an ensemble using all models  , we uniformly sampled , with replacement , from this pool models for inclusion into thenal ensemble  . Random selection introduces variation , so we repeated our results ten time and averaged the results  . 
Figure 3 shows our results ( N is the number of models in each ensemble , x is the mean performance and  ( marked all ) shows the performance obtained using an ensemble consisting of all models in the model  , equally weighted . 
As can be seen , increasing the number of models in the ensemble reduces the inuence of the initial weight settings  . However , even with a large pool of models , we still marginally underperform uniform initialisation  . 

Running IIS until convergence did not narrow the gap  , so our ndings cannot be attributed to dierential rates of convergence  . 
Nx  241 . 41 1 . 04 100 50 . 55 0 . 70 3 44 . 24 1 . 63 150 50 . 62 0 . 88 5 46 . 57 1 . 69 200 50 . 66 0 . 75 10 47 . 21 1 . 71 300 50 . 81 0 . 73 20 49 . 23 1 . 19 600 51 . 04 0 . 36 all 51 . 39 -
Figure 3: Model averaging results
Note that we have not carried out the control experiment of repeating our runs using features which do not overlap with each other  . Unfortunately , doing this would probably mean having to create models that are unnatural  . We therefore cannot be absolutely certain that the sole reason for the existance of local optima is the presence of overlapping features  . However , we can be sure that they do exist , and that varying the initial weight settings will reveal them  . 
8 Comments
We have established that , in the presence of overlapping features , the values of the initial weights can aect the utility of the nal model  . Our experimental results support this nding . In general , one might encounter a similar problem ( overlapping features ) with what might be called ` semi-overlapping features ': features which are very similar  ( but not identical ) on the training data and very dierent outside of it  . 
IIS could be made more robust to the choice of initial parameters in a number of ways :  The simplest course of action is to set al initial weights to the same value  . Although zero is often a convenient initial value  , in principle any real number would do , since the IIS algorithm can reach a given optimal solution from any starting point in the space of initial parameters  . 
 We could also examine all the features to determine which ones overlap  , and force it to balance then alweights of these features  . For very large models , this may be prohibitively di ? cult and time -consuming  . 
 Model averaging can also cancel out variations caused by a particular choice of initial settings  . 
However , this implies a greater computational burden as IIS will need to be run many times in order to gain a representative sample of models  . 
 The number of features in the model could be reduced using feature selection methods  ( for example ( Mullen and Osborne ,  2000)) . 
Although IIS is a useful tool for estimating loglinear models  , we have since moved-on to estimating models using limited-memory variable-metric methods  ( Malouf ,  2002) . Our ndings show that convergence , for a range of problems , is faster . An interesting question is seeing the extent to which other numerical methods for estimating loglinear models are sensitive to initial parameter values  . Finally , it should be noted that our theoretical results apply to a more general setting than that of loglinear models trained using the IIS algorithm  . The problem of overlapping features could in principle occur in any situation in which a model has a linear combination of features  , and a ` hillclimbing ' algorithm is used to seek a maximum-likelihood solution  . 

We wish to thank Steve Clark for useful discussions about IIS  , RobMalouf for supplying the IIS implementation and the two anonymous reviewers  . Iain Bancarz was supported by the EPSRC grant POEM  . 

Steven P . Abney .  1997 . Stochastic Attribute Value Grammars . Computational Linguistics ,  23(4):597618 , December . 
Ted Briscoe and John Carroll .  1996 . Automatic Extraction of Subcategorization from Corpora  . 
In Proceedings of the 5th
Conference on Applied
NLP , pages 356363, Washington , DC.
Mark Johnson , Stuart Geman , Stephen Cannon , Zhiyi Chi , and Stephan Riezler .  1999 . Estimators for Stochastic\Unication-Based " Grammars  . In
Annual Meeting of the ACL.
J . Laerty , S . Della Pietra , and V . Della Pietra . 
1997 . Inducing features of randomelds . IEEE Transactions on Pattern Analysis and Machine
Intelligence , 19(4):380393, April.
Robert Malouf .  2002 . A comparison of algorithms for maximum entropy parameter estimation  . In Proceedings of the joint CoNLL-WVLC Meeting,
Taipei , Taiwan . ACL . To appear.
Tony Mullen and Miles Osborne .  2000 . Overtting Avoidance for Stochastic Modeling of Attribute Value Grammars  . In Claire Cardie , Walter Daelemans , Claire Nedellec , and Erik Tjong Kim Sang , editors , Proceedings of the Computational Natural Language learning  2000  , pages 4954 . ACL , Lisbon , Portugal . 
Kamal Nigam , John Laerty ,   , and Andrew McCallum .  1999 . Using maximum entropy for text classication . In IJCAI99 Workshop on Machine
Learning for Information Filtering,.
Miles Osborne .  2000 . Estimation of Stochastic Attribute Value Grammars using an Informative 
Sample . In The 18th
International Conference on
Computational Linguistics , Saarbr ucken , August.
