Proceedings of the 23rd International Conference on Computational Linguistics ( Coling 2010), pages 1173?1181,
Beijing , August 2010
A Character-Based Joint Model for Chinese Word Segmentation
Kun Wang and Chengqing Zong
National Laboratory of Pattern Recognition
Institute of Automation , Chinese Academy of Science
{kunwang,cqzong}@nlpr.ia.ac.cn
Keh-Yih Su
Behavior Design Corporation

Kysu@bdc.com.tw

Abstract
The character-based tagging approach is a dominant technique for Chinese word segmentation , and both discriminative and generative models can be adopted in that framework . However , generative and discriminative charac-ter-based approaches are significantly different and complement each other.
A simple joint model combining the character-based generative model and the discriminative one is thus proposed in this paper to take advantage of both approaches . Experiments on the Second SIGHAN Bakeoff show that this joint approach achieves 21% relative error reduction over the discriminative model and 14% over the generative one.
In addition , closed tests also show that the proposed joint model outperforms all the existing approaches reported in the literature and achieves the best Fscore in four out of five corpora.
1 Introduction
Chinese word segmentation ( CWS ) plays an important role in most Chinese NLP applications such as machine translation , information retrieval and question answering . Many statistical methods for CWS have been proposed in the last two decades , which can be classified as either word-based or character-based . The word-based approach regards the word as the basic unit , and the desired segmentation result is the best word sequence found by the search process . On the other hand , the character-based approach treats the word segmentation task as a character tagging problem . The final segmentation result is thus indirectly generated according to the tag assigned to each associated character . Since the vocabulary size of possible character-tag-pairs is limited , the character-based models can tolerate out-of-vocabulary ( OOV ) words and have become the dominant technique for CWS in recent years.
On the other hand , statistical approaches can also be classified as either adopting a generative model or adopting a discriminative model.
The generative model learns the joint probability of the given input and its associated label sequence , while the discriminative model learns the posterior probability directly . Generative models often do not perform well because they make strong independence assumptions between features and labels . However , ( Toutanova , 2006) shows that generative models can also achieve very similar or better performance than the corresponding discriminative models if they have a structure that avoids unrealistic independence assumptions.
In terms of the above dimensions , methods for CWS can be classified as : 1) The word-based generative model ( Gao et al ., 2003; Zhang et al , 2003), which is a wellknown approach and has been used in many successful applications ; 2) The word-based discriminative model ( Zhang and Clark , 2007), which generates word candidates with both word and character features and is the only word-based model that adopts the discriminative approach ? 3) The character-based discriminative model ( Xue , 2003; Peng et al , 2004; Tseng et al , 2005; Jiang et al , 2008), which has become the dominant method as it is robust on OOV words and is capable of handling a range of different features , and it has been adopted in many previous works ; ( Wang et al , 2009), which adopts a character-tag-pair-based ngram model and achieves comparable results with the popular character-based discriminative model.
In general , character-based models are much more robust on OOV words than word-based approaches do , as the vocabulary size of characters is a closed set ( versus the open set of that of words ). Furthermore , among those character-based approaches , the generative model and the discriminative one complement each other in handling in-vocabulary ( IV ) words and OOV words . Therefore , a character-based joint model is proposed to combine them.
This proposed joint approach has achieved good balance between IV word recognition and OOV word identification . The experiments of closed tests on the second SIGHAN Bakeoff ( Emerson , 2005) show that the joint model significantly outperforms the baseline models of both generative and discriminative approaches . Moreover , statistical significance tests also show that the joint model is significantly better than all those state-of-the-art systems reported in the literature and achieves the best Fscore in four of the five corpora tested.
2 Character-Based Models for CWS
The goal of CWS is to find the corresponding word sequence for a given character sequence.
Character-based model is to find out the corresponding tags for given character sequence.
2.1 Character-Based Discriminative Model The character-based discriminative model ( Xue , 2003) treats segmentation as a tagging problem , which assigns a corresponding tag to each character . The model is formulated as : 1 1 ( ) ( , ) ( n n n n k n k k k k k
P t c P t t c P t c ? ? = = = ?? ? 2 ) k + (1) Where tk is a member of { Begin , Middle , End , Single } ( abbreviated as B , M , E and S from now on ) to indicate the corresponding position of character ck in its associated word . For example , the word ???? ( Beijing City )? will be assigned with the corresponding tags as : ?? / B ( North ) ?/ M ( Capital ) ?/ E ( City)?.
Since this tagging approach treats characters as basic units , the vocabulary size of those possible character-tag-pairs is limited . Therefore , this method is robust to OOV words and could possess a high recall of OOV words ( ROOV ). Although the dependency between adjacent tags/labels can be addressed , the dependency between adjacent characters within a word cannot be directly modeled under this framework . Lower recall of IV words ( RIV ) is thus usually accompanied ( Wang et al , 2009).
In this work , the character-based discriminative model is implemented by adopting the feature templates given by ( Ng and Low , 2004), but excluding those ones that are forbidden by the closed test regulation of SIGHAN ( e.g ., Pu(C0): whether C0 is a punctuation ). Those feature templates adopted are listed below : ( ) ( 2, 1,0,1, 2); ( ) ( 2, 1,0,1); ( ) n n n a C n b C C n c C C + ? = ? ? = ? ? For example , when we consider the third character ??? in the sequence ???????, template ( a ) results in the features as following : C2=?, C1=?, C0=?, C1=?, C2=?, and template ( b ) generates the features as : C-2C-1=??,
C1C0=??, C0C1=??, C1C2=??, and template ( c ) gives the feature C-1C1=??.
2.2 Character-Based Generative Model
To incorporate the dependency between adjacent characters in the character-based approach , ( Wang et al , 2009) proposes a character-based generative model . In this approach , word wi is first replaced with its corresponding sequence of [ character , tag ] ( denoted as [ c , t ]), where tag is the same as that adopted in the above char-acter-based discriminative model . With this representation , this model can be expressed as : 1 1 1 1 1 1 1 1 ( ) ([ , ] ) ( [ , ] ) ([ , ] ) ( ) m n n n n n n n
P w c P c t c
P c c t P c t P c ? = ? (2)
Since 1 1( [ , ] ) 1 n nP c c t ? and is the same for various candidates , only should be considered . It can be further simplified with
Markov Chain assumption as : 1( ) nP c ([ , P c 1] ) nt ([ , ] ) ([ , ] [ , ] ).
n n i i k i
P c t P c t c t ?? = ?? i (3)
Compared with the character-based discriminative model , this generative model keeps the capability to handle OOV words because it also regards the character as basic unit . In addition , the dependency between adjacent Tag probability : B/0.0333 E/0.2236 M/0.7401 S/0.0030
Feature
Tag C2 C1 C0 C1 C2 C-2C-1 C1C0 C0C1 C1C2 C-1C1 B -1.4375 0.1572 0.0800 0.2282 0.7709 0.2741 0.0000 0.0000 -0.6718 0.0000 E 1.3558 0.1910 0.7229 -1.2696 -0.5970 0.0049 0.0921 0.0000 0.8049 0.0000 M 1.1071 -0.5527 -0.3174 2.9422 0.4636 -0.1708 0.0000 0.0000 -0.9700 0.0000 S -1.0254 0.2046 -0.4856 -1.9008 -0.6375 0.0000 0.0000 0.0000 0.8368 0.0000 ? Gold and Discriminative Tag : E Generative Trigram Tag : S Tag probability : B/0.0009 E/0.8138 M/0.0012 S/0.1841
Feature
Tag C2 C1 C0 C1 C2 C-2C-1 C1C0 C0C1 C1C2 C-1C1 B 0.3586 0.4175 0.0000 -0.7207 0.4626 0.0085 0.0000 0.0000 0.0000 0.0000 E 0.3666 0.0687 4.5381 2.8300 -0.0846 0.0000 0.0000 -1.0279 0.6127 0.0000 M -0.5657 -0.4330 1.8847 0.0000 -0.0918 0.0000 0.0000 0.0000 0.0000 0.0000 S -0.1595 -0.0532 2.7360 1.8223 -0.2862 -0.0024 0.0000 1.0494 0.7113 0.0000 Table 1: The corresponding lambda weight of features for ????? in the sentence ?[?] [?] [?] [???] [?] [?] [?] [?]?. In the Feature column and Tag row , the value is the corresponding lambda weight for the feature and tag under ME framework . The meanings of those features are explained in Section 2.1.
characters is now directly modeled . This will give sharper preference when the history of assignment is given . Therefore , this approach not only holds robust IV performance but also achieves comparable results with the discriminative model . However , the OOV performance of this approach is still lower than that of the discriminative model ( see in Table 5), which would be discussed in the next section.
3 Problems with the Character-Based
Generative Model
The character-based generative model can handle the dependency between adjacent characters and thus performs well on IV words.
However , this generative trigram model is derived under the second order Markov Chain assumption . Future character context ( i.e ., C1 and C2) is thus not utilized in the model when the tag of the current character ( i.e ., t0) is determined . Nevertheless , the future context would help to select the correct tag when the associated trigram has not been observed in the trainingset , which is just the case for those OOV words . In contrast , the discriminative one could get help from the future context in this case . The example given in the next paragraph clearly shows the above situation.
At the sentence ??( that ) ?( place ) ?( of ) ? ??( street sleeper ) ?( only ) ?( have ) ?( some ) ?( person ) ( There are only some street sleepers in that place )? in the CITYU corpus , ??/ B ? / M?/E(street sleeper )? is observed to be an OOV word , while ?? / B ? / E(sleep on the street )? is an IV word , where the associated tag of each character is given after the slash symbol . The character-based generative model wrongly splits ????? into two words ??/ B ?/ E ? and ??/ S ( person )?, as the associated trigram for ????? is not seen in the training set . However , the discriminative model gives the correct result for ??/ M ? and the dominant features come from its future context ??? and ???. Similarly , the future context ??? helps to give the correct tag to ??/ E ?. Table 1 gives the corresponding lambda feature weights ( under the Maximum Entropy ( ME ) ( Ratnaparkhi , 1998) framework ) for ????? in the discriminative model . It shows that in the column of ? C1? below ???, the lambda value associated with the correct tag ? M ? is 2.9422, which is the highest value in that column and is far greater than that of the wrong tag ? E ? ( i.e ., -1.2696) assigned by the generative model.
Which indicates that the future feature ? C1? is the most useful feature for tagging ???.
The above example shows the character-based generative model fails to handle some OOV words such as ????? because this approach cannot utilize future context when it is indeed required . However , the future context for the generative model scanning from left to right is just its past context when it scans from right to left . It is thus expected that this kind of from both directions , and then combine their results . Unfortunately , it is observed that these two scanning modes share over 90% of their errors . For example , in CITYU corpus , the left-to-right scan generates 1,958 wrong words and the right-to-left scan results 1,947 ones , while 1,795 of them are the same . Similar behavior can also be observed on other corpora.
To find out what are the problems , 10 errors that are similar to ????? are selected to examine . Among those errors , only one of them is fixed , and ????? still cannot be correctly segmented . Having analyzed the scores of the model scanning from both directions , we found that the original scores ( from left-to-right scan ) at the stages ??? and ??? indeed get better if the model scans from right-to-left . However , the score at the stage ??? deteriorates because the useful feature ??? ( a past nonadjacent character for ??? when scans form right-to-left ) still cannot be utilized when the past context ???? as a whole is unseen , when the related probabilities are estimated via modified Kneser-Ney smoothing ( Chen and Goodman , 1998) technique.
Two scanning modes seem not complementing each other , which is out of our original expectation . However , we found that the charac-ter-based generative model and the discriminative one complement each other much more than the two scanning modes do . It is observed that these two approaches share less than 50% of their errors . For example , in CITYU corpus , the generative approach generates 1,958 wrong words and the discriminative one results 2,338 ones , while only 835 of them are the same.
The statistics of the remaining errors resulted from the generative model and the discriminative model is shown in Table 2. As shown in the table , it can be seen that the generative model and the discriminative model complement each other on handling IV words and OOV words ( In the ? IV Errors ? column , the number of ? G+D -? is much more than the ? GD +?, while the behavior is reversed in the ? OOV Errors ? column).
4 Proposed Joint Model
Since the performance of both IV words and OOV words are important for real applications,
IV Errors OOV Errors
G+D - GD + GD - G+D - GD + G-D-12,027 4,723 7,481 2,384 6,139 3,975 Table 2: Statistics for remaining errors of the char-acter-based generative model and the discriminative one on the second SIGHAN Bakeoff (? G+D -? in the ? IV Errors ? column means that the generative model segments the IV words correctly but the discriminative one gives wrong results . The meanings of other abbreviations are similar with this one.).
we need to combine the strength from both models . Among various combining methods , loglinear interpolation combination is a simple but effective one ( Bishop , 2006). Therefore , the following character-based joint model is proposed , and a parameter ? is used to weight the generative model in a crossvalidation set.
( ) log ( ([ , ] [ , ] )) (1 ) log ( ( )) k k k k k k
Score t P c t c t
P t c ? ? ? ? + ? = ? + ? ? k (4) Where tk indicates the corresponding position of character ck , and (0.0 1.0)? ?? ? is the weight for the generative model . Score(tk ) will be used during searching the best sequence . It can be seen that these two models are integrated naturally as both are character-based.
Generally speaking , if the ? G(or D )+? has a strong preference on the desired candidate , but the ? D(or G )-? has a weak preference on its top1 incorrect candidate , then this combining method would correct most ? G+D - ( also GD +)? errors . On the other hand , the advantage of combining two models would vanish if the ? G(or D )+? has a weak preference while the ? D(or G )-? has a strong preference over their top1 candidates . In our observation , these two models meet this requirement quite well.
5 Weigh Various Features Differently
For a given observation , intuitively each feature should be trained only once under the ME framework and its associated weight will be automatically learned from the training corpus . However , when we repeat the work of ( Jiang et al , 2008), which reports to achieve the state-of-art performance in the datasets that we adopt , it has been found that some features ( e.g ., C0) are unnoticeably trained several times in their model ( which are implicitly generated from different feature templates used in the paper ). For example , the feature C0 actually
Test Size ( Words/Type ) OOV Rate
Academia Sinica ( Taipei ) AS Unicode/Big5 5.45M/141K 122K/19K 0.046 City University of Hong Kong CITYU Unicode/Big5 1.46M/69K 41K/9K 0.074 Microsoft Research ( Beijing ) MSR Unicode/CP936 2.37M/88K 107K/13K 0.026 PKU(ucvt .) Unicode/CP936 1.1M/55K 104K/13K 0.058 Peking University PKU(cvt .) Unicode/CP936 1.1M/55K 104K/13K 0.035 Table 3: Corpus statistics for the second SIGHAN Bakeoff appears twice , which is generated from two different templates Cn ( with n=0, generates C0) and [ C0Cn ] ( used in ( Jiang et al , 2008), with n=0, generates [ C0C0]). The meanings of features are illustrated in Section 2.1. Those repetitive features also include [ C1C0] and [ C0C1], which implicitly appear thrice . And it is surprising to discover that its better performance is mainly due to this implicit feature repetition but the authors do not point out this fact.
As all the features adopted in ( Jiang et al , 2008) possess binary values , if a binary feature is repeated n times , then it should behave like a real-valued feature with its value to be ? n ?, at least in principle . Inspired by the above discovery , accordingly , we convert all the binary-value features into their corresponding real-valued features . After having transformed binary features into their corresponding real-valued ones , the original discriminative model is retrained under the ME framework.
This new implementation , which would be named as the character-based discriminative-plus model , just weights various features differently before conducting ME training . Afterwards , it is further combined with the generative trigram model , and is called the charac-ter-based joint-plus model.
6 Experiments
The corpora provided by the second SIGHAN Bakeoff ( Emerson , 2005) were used in our experiments . The statistics of those corpora are shown in Table 3.
Note that the PKU corpus is a little different from others . In the training set , Arabic numbers and English characters are in fullwidth form occupying two bytes . However , in the testing set , these characters are in half-width form occupying only one byte . Most researchers in the SIGHAN Bakeoff competition performed a conversion before segmentation ( Xiong et al , 2009). In this work , we conduct the tests on both unconverted ( ucvt .) case and converted ( cvt .) case . After the conversion , the OOV rate of converted corpus is obviously lower than that of unconverted corpus.
To fairly compare the proposed approach with previous works , we only conduct closed tests1. The metrics Precision ( P ), Recall ( R ), Fscore ( F ) ( F=2PR/(P+R )), Recall of OOV ( ROOV ) and Recall of IV ( RIV ) are used to evaluate the results.
6.1 Character-Based Generative Model and Discriminative Model As shown in ( Wang et al , 2009), the character-based generative trigram model significantly exceeds its related bigram model and performs the same as its 4gram model . Therefore , SRI Language Modeling Toolkit2 ( Stolcke , 2002) is used to train the trigram model with modified Kneser-Ney smoothing ( Chen and Goodman , 1998). Afterwards , a beam search decoder is applied to find out the best sequence.
For the character-based discriminative model , the ME Package3 given by Zhang Le is used to conduct the experiments . Training was done with Gaussian prior 1.0 and 300, 150 iterations for AS and other corpora respectively.
Ta ble 5 gives the segmentation results of both the character-based generative model and the discriminative model . From the results , it can be seen that the generative model achieves comparable results with the discriminative one and they outperform each other on different corpus . However , the generative model exceeds the discriminative one on RIV (0.973 vs.
0.956) but loses on ROOV (0.511 vs . 0.680). It illustrates that they complement each other.
1 According to the second Sighan Bakeoff regulation , the closed test could only use the training data directly provided . Any other data or information is forbidden , including the knowledge of characters set , punctuation set , etc.
2 http://www.speech.sri.com/projects/srilm / 3 http://homepages.inf.ed.ac.uk/lzhang10/maxent_toolkit.html 0.9300 0.9400 0.9500 0.9600 0.9700 0.9800 0.9900 0.0
Fsc or e
AS
CITYU
MSR
PKU
Figure 1: Development sets performance of Charac-ter-based joint model.
Corpus Set Words OOV Num OOV Rate
Development 17,243 445 0.026 AS
Testing 122,610 5,308/5,311 0.043/0.043
Development 17,324 355 0.020 MSR
Testing 106,873 2,829/2,833 0.026/0.027
Development 12,075 537 0.044 CITYU
Testing 40,936 3,028/3,034 0.074/0.074
Development 13,576 532 0.039
Testing ( ucvt .) 104,372 6,006/6,054 0.058/0.058PKU Testing ( cvt .) 104,372 3,611/3,661 0.035/0.035 Table 4: Corpus statistics for Development sets and Testing sets . A ?/? separates the OOV number ( or OOV rate ) with respect to the original training sets and the new training sets.
6.2 Character-Based Joint Model
For the character-based joint model , a development set is required to obtain the weight ? for its associated generative model . A small portion of each original training corpus is thus extracted as the development set and the remaining data is regarded as the new trainingset , which is used to train two new parameter-sets for both generative and discriminative models associated.
The last 2,000, 600, 400, and 300 sentences for AS , MSR , CITYU , and PKU are extracted from the original training corpora as their corresponding development sets . The statistics for new data sets are shown in Table 4. It can be seen that the variation of the OOV rate could be hardly noticed . The Fscores of the joint model , versus different ? , evaluated on four development sets are shown in Figure 1. It can be seen that the curves are not sharp but flat near the top , which indicates that the character-based joint model is not sensitive to the ? value selected . From those curves , the best suitable ? for AS , CITYU , MSR and PKU are found to be 0.30, 0.60, 0.60 and 0.60, respec-
Corpus Model R P F ROOV RIV tively . Those alpha values will then be adopted to conduct the experiments on the testing sets.
G 0.958 0.938 0.948 0.518 0.978
D 0 0.946 0 0.967.955 .951 0.707
D-Plus 0.960 0.948 0.954 0.680 0.973
J 0.962 0.950 0.956 0.679 0.975
AS
J-Plus 0.963 0.949 0.956 0.652 0.977
G 0.951 0.937 0.944 0.609 0.978
D 0.941 0.944 0.942 0.708 0.959
D-Plus 0.951 0.952 0.952 0.720 0.970
J 0.957 0.951 0.954 0.691 0.979
CITYU
J-Plus 0.959 0.952 0.956 0.700 0.980
G 0.974 0.967 0.970 0.561 0.985
D 0.957 0.962 0.960 0.719 0.964
D-Plus 0.965 0.967 0.966 0.675 0.973
J 0.974 0.971 0.972 0.659 0.983
MSR
J-Plus 0.975 0.970 0.972 0.632 0.984
G 0.929 0.933 0.931 0.435 0.959
D 0.922 0.941 0.932 0.620 0.941
D-Plus 0.934 0.949 0.941 0.649 0.951
J 0.935 0.946 0.941 0.561 0.958
PKU ( ucvt.)
J-Plus 0.937 0.947 0.942 0.556 0.960
G 0.952 0.951 0.952 0.503 0.968
D 0.940 0.951 0.946 0.685 0.949
D-Plus 0.949 0.958 0.953 0.674 0.958
J 0.954 0.958 0.956 0.616 0.966
PKU ( cvt.)
J-Plus 0.955 0.958 0.957 0.610 0.967
G 0.953 0.946 0.950 0.511 0.973
D 0.944 0.950 0.947 0.680 0.956
D-Plus 0.952 0.955 0.953 0.676 0.965
J 0.957 0.955 0.956 0.633 0.971
Overall
J-Plus 0.958 0.955 0.957 0.621 0.973
Table 5: ent e based m n t G ificantly outperforms both the character-ba Segm odels o ation r sults of various character-he second SI HAN Bakeoff , the generative trigram model ( G ), the discriminative model ( D ), the discriminative-plus model ( D-Plus ), the joint model ( J ) and the joint-plus model ( J-Plus).

As shown in Table 5, the joint model sign sed generative model and the discriminative one in Fscore on all the testing corpora . Compared with the generative approach , the joint model increases the overall ROOV from 0.510 to 0.633, with the cost of slightly degrading the overall RIV from 0.973 to 0.971. This shows that the joint model holds the advantage of the generative model on IV words . Compared with the discriminative model , the proposed joint model improves the overall RIV from 0.956 to 0.971, with the cost of degrading the overall ROOV from 0.680 to 0.633. It clearly shows that the joint model achieves a good balance between IV words and OOV words and achieves the best Fscores obtained so far (21% relative error reduction over the discriminative model and 14% over the generative model).
1178 6.3 Weigh Various Features Differently Inspired by ( Jiang et al , 2008), we set the real-d Although Table 5 has shown that the proposed all the value of C0 to be 2.0, the value of C1C0 an C0C1 to be 3.0, and the values of all other features to be 1.0 for the character-based dis-criminative-plus model . Although it seems reasonable to weight those closely relevant features more ( C0 should be the most relevant feature for assigning tag t0), both implementations seem to be equal if their corresponding lambda-values are also updated accordingly.
However , Table 5 shows that this new dis-criminative-plus implementation ( D-Plus ) significantly outperforms the original one ( overall Fscore is raised from 0.947 to 0.953) when both of them adopt real-valued features . It is not clear how this change makes the difference.
Similar improvements can be observed with two other ME packages . One anonymous reviewer pointed out that the duplicated features should not make difference if there is no regularization . However , we found that the duplicated features would improve the performance whether we give Gaussian penalty or not.
Afterwards , this new implementation and the generative trigram model are further combined ( named as the joint-plus model ). Table 5 shows that this joint-plus model also achieves better results compared with the discrimina-tive-plus model , which illustrates that our joint approach is an effective and robust method for CWS . However , compared with the original joint model , the new joint-plus approach does not show much improvement , regardless of the significant improvement made by the discrimi-native-plus model , as the additional benefit generated by the discriminative-plus model has already covered by the generative approach ( Among the 6,965 error words corrected by the discriminative-plus model , 6,292 (90%) of them are covered by the generative model).
7 Statistical Significance Tests joint ( joint-plus ) model outperforms baselines mentioned above , we want to know if the difference is statistically significant enough to make such a claim . Since there is only one testing set for each training corpus , the bootstrapping technique ( Zhang et al , 2004) is adopted to conduct the tests : Giving an
Models
A B
AS CITYU MSR PKU ( ucvt.)
PKU ( cvt.)
G D < ~ > ~ >
D-Plus G > > < > >
D-Plus D > > > > >
J G > > > > >
J D > > > > >
J-Plus G > > > > >
J-Plus D-Plus > > > ~ >
J-Plus J ~ > ~ > >
Table 6 atistic sign anc est F - e v erb d m ls.
f T0) will be generated by repeatedly resampling data eas - the dis-he confi-the propo eng d.
tegory includes ( Asahara et al , 2005) ( denoted as : St al ific e t of scor among arious charact ase ode testing-set T0, additional M1 new testing-sets
T0,?,TM-1 ( each with the same size o from T0. Then , we will have a total of M testing-sets ( M=2000 in our experiments).
7.1 Comparisons with Baselines
We then follow ( Zhang et al , 2004) to m ure the 95% confidence interval for crepancy between two models . If t dence interval does not include the origin point , we then claim that system A is significantly different from system B . Table 6 gives the results of significant tests among various models mentioned above . In this table , ?>? means that system A is significantly better than B , where as ?<? denotes that system A is significantly worse than B , and ?~? indicates that these two systems are not significantly different.
As shown in Table 6, the proposed joint model is significantly better than the two baseline models on all corpora . Similarly , sed joint-plus model also significantly outperforms the generative model and the dis-criminative-plus model on all corpora except on the PKU(ucvt .). The comparison shows that the proposed joint ( also joint-plus ) model indeed exceeds each of its component models.
7.2 Comparisons with Previous Works
The above comparison mainly shows the sup riority of the proposed joint model amo those approaches that have been implemente However , it would be interesting to know if the joint ( and joint-plus ) model also outperforms those previous state-of-the-art systems.
The systems that performed best for at least one corpus in the second SIGHAN Bakeoff are first selected for comparison . This ca st th sahara05) and ( Tseng et al , 2005) 4 ( Tseng05). ( Asahara et al , 2005) achieves the best result in the AS corpus , and ( Tseng et al , 2005) performs best in the remaining three corpora . Besides , those systems that are reported to exceed the above two systems are also selected . This category includes ( Zhang et al ., 2006) ( Zhang06), ( Zhang and Clark , 2007) ( Z&C07) and ( Jiang et al , 2008) ( Jiang08).
They are briefly summarized as follows.
(Zhang et al , 2006) is based on subword tagging and uses a confidence measure method to combine the subword CRF ( Lafferty et al , 2001) and rule-based models . ( Zhang and Clark , 2007) uses perceptron ( Collins , 2002) to generate word candidates with both word and character features . Last , ( Jiang et al , 2008)5 adds repeated features implicitly based on ( Ng and Low , 2004). All of the above models , except ( Zhang and Clark , 2007), adopt the char-acter-based discriminative approach.
All the results of the systems mentioned above are shown in Table 7. Since the systems are not reimplemented , we cannot generate paired samples from those M testing ead , we calculate the 95% confidence interval of the joint ( also joint-plus ) model . Afterwards , those systems can be compared with our proposed models . If the Fscore of system B does not fall within the 95% confidence interval of system A ( joint or joint-plus ), then they are statistically significantly different.
Table 8 gives the results of significant tests for those systems mentioned in this section . It shows that both our joint-plus model and joint model exceed ( or are comparable to ) almost all e state-of-the-art systems across all corpora , except ( Zhang and Clark , 2007) at PKU(ucvt.).
In that special case , ( Zhang and Clark , 2007) 4 We are not sure whether ( Asahara et al , 2005) and ( Tseng et al , 2005) performed a conversion before segmentation in PKU corpus . In this paper , we followed previous works , which cited and compared with them.
5 The data for ( Jiang et al , 2008) given at Table 7 are different from what were reported at their paper . In the communication with the authors , it is found that the script for evaluating performance , provided by the SIGHAN Bakeoff , does not work correctly in their platform . After the problem is fixed , the reevaluated real performances reported here deteriorate from their original version.
Please see the announcement in Jiang?s homepage ( http://mtgroup.ict.ac.cn/~jiangwenbin/papers/error_corre ction.pdf).
Corpus
Participants AS CITYU MSR
PKU ( ucvt.)
PKU ( cvt.)
Asahara05 0.952 0.941 0.958 N/A 0.941
Tseng05 0.947 0.943 0.964 N/A 0.950
Zhang06 0.951 0.951 0.971 N/A 0.951
Z&C07 0.946 0.951 0.972 0.945 N/A
Jiang08 0.953 0.948 0.966 0.937 N/A
Our Joint 0.956 0.954 0.972 0.941 0.956
Our Joint-Plus 0.956 0.956 0.972 0.942 0.957
Table 7: Compari r p u the-art sy sons of F-sco e with revio s state-of - stems.
Systems
A B
AS CITYU MSR ( ucvt.)
PKU ( cvt.)
PKU
Asahara05 > > > N/A >
Tseng05 > > > N/A >
Zhang06 > ~ ~ N/A >
Z&C07 > > ~ < N/A
J
Jiang08 > > > > N/A
Asahara05 > > > N/A >
Tseng05 > > > N/A >
Zhang06 > > ~ N/A >
Z&C07 > > ~ < N/A
J-Plus
Jiang08 ~ > > > N/A
Table al s ific e te of r fthe syst s.
outpe he jo - plu model by .3% and 0.5%, re-ne , e two models complement dling IV words and OOV e-nomenon.
8: Statistic ign anc st Fscore fo previous state-o - art em rforms t int s 0 on F - score (0.4% for the joint model ). However , our joint-plus model exceeds it more over AS and CITYU corpora by 1.0% spectively (1.0% and 0.3% for the joint model).
Thus , it is fair to say that both our joint model and joint-plus model are superior to the state-of-the-art systems reported in the literature.
8 Conclusion
From the error analysis of the character-based generative model and the discriminative o we found that thes each other on han words . To take advantage of these two approaches , a joint model is thus proposed to combine them . Experiments on the Second SIGHAN Bakeoff show that the joint model achieves 21% error reduction over the discriminative model (14% over the generative model ). Moreover , closed tests on the second SIGHAN Bakeoff corpora show that this joint model significantly outperforms all the state-of-the-art systems reported in the literature.
Last , it is found that weighting various features differently would give better result . However , further study is required to find out the true reason for this strange but interesting ph o Ms . Nanyan Kuo for eric-Beam-Search code.
m optimum entation . In Proceedings of
GHAN Workshop on Chinese Lan-
St
Th
Jia
W
Jo
Hw
Fu ction using conditional random fields.
Ad
MNLP , pages
Hu ld Word Segmenter
Ku d Keh-Yih Su , 2009.
Yi scriminative
Ni ssing , 8 (1). pages
Hu
Second
Ru 2006. Subword-based Tagging for Con-
Yi scores : How much im-
Yu f ACL , pages 840-847, cknowledgement
The authors extend sincere thanks to Wenbing Jiang for his helps with our experiments . Also , we thank Behavior Design Corporation for using their show special thanks t her helps with the Gen The research work has been partially funded by the Natural Science Foundation of China under Grant No . 60975053, 90820303 and 60736014, the National Key Technology R&D
Program under Grant No . 2006BAH03B02, and also the HiTech Research and Develop-ent Program (?863? Program ) of China under
Grant No . 2006AA010108-4 as well.
References
Masayuki Asahara , Kenta Fukuoka , Ai Azuma , Chooi-Ling Goh , Yotaro Watanabe , Yuji Matsumoto and Takashi Tsuzuki , 2005. Combination of machine learning methods for
Chinese word segm the Fourth SI guage Processing , pages 134?137, Jeju , Korea.
Christopher M . Bishop , 2006. Pattern recognition and machine learning . New York : Springer anley F . Chen and Joshua Goodman , 1998. An empirical study of smoothing techniques for language modeling . Technical Report TR-10-98, Harvard University Center for Research in
Computing Technology.
Michael Collins , 2002. Discriminative training methods for hidden markov models : theory and experiments with perceptron algorithms . In Proceedings of EMNLP , pages 18, Philadelphia.
omas Emerson , 2005. The second international Chinese word segmentation bakeoff . In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing , pages 123-133.
nfeng Gao , Mu Li and Chang-Ning Huang , 2003.
Improved Source-Channel Models for Chinese Word Segmentation . In Proceedings of ACL , pages 272-279.
enbin Jiang , Liang Huang , Qun Liu and Yajuan Lu , 2008. A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging . In Proceedings of ACL , pages 897-904.
hn Lafferty , Andrew McCallum and Fernando Pereira , 2001. Conditional Random Fields : Probabilistic Models for Segmenting and Labeling Sequence Data . In Proceedings of ICML , pages 282-289.
ee Tou Ng and Jin Kiat Low , 2004. Chinese part-of-speech tagging : one-at-a-time or all-at-once ? word-based or character-based . In Proceedings of EMNLP , pages 277-284.
chun Peng , Fangfang Feng and Andrew
McCallum , 2004. Chinese segmentation and new word dete In Proceedings of COLING , pages 562?568.
wait Ratnaparkhi , 1998. Maximum entropy models for natural language ambiguity resolution . University of Pennsylvania.
Andreas Stolcke , 2002. SRILM-an extensible language modeling toolkit . In Proceedings of the International Conference on Spoken Language
Processing , pages 311-318.
Kristina Toutanova , 2006. Competitive generative models with structure learning for NLP classification tasks . In Proceedings of E 576-584, Sydney , Australia.
ihsin Tseng , Pichuan Chang , Galen Andrew , Daniel Jurafsky and Christopher Manning , 2005.
A Conditional Random Fie for Sighan Bakeoff 2005. In Proceedings of the
Fourth SIGHAN Workshop on Chinese Language Processing , pages 168-171.
n Wang , Chengqing Zong an
Which is more suitable for Chinese word segmentation , the generative model or the discriminative one ? In Proceedings of PACLIC , pages 827-834, Hong Kong , China.
ng Xiong , Jie Zhu , Hao Huang and Haihua Xu , 2009. Minimum tag error for di training of conditional random fields . Information Sciences , 179 (12). pages 169-179.
anwen Xue , 2003. Chinese Word Segmentation as Character Tagging . Computational Linguistics and Chinese Language Proce 2948.
aping Zhang , Hongkui Yu , Deyi Xiong and Qun Liu , 2003. HHMM-based Chinese lexical analyzer ICTCLAS . In Proceedings of the SIGHAN Workshop on Chinese Language Processing , pages 184?187.
iqiang Zhang , Genichiro Kikui and Eiichiro
Sumita , fidence-dependent Chinese Word Segmentation.
In Proceedings of the COLING/ACL , pages 961-968, Sydney , Australia.
ng Zhang , Stephan Vogel and Alex Waibel , 2004.
Interpreting BLEU/NIST provement do we need to have a better system.
In Proceedings of LREC , pages 2051?2054.
e Zhang and Stephen Clark , 2007. Chinese Segmentation with a Word-Based Perceptron Algorithm . In Proceedings o
Prague , Czech Republic.
1181
