Proceedings of the 23rd International Conference on Computational Linguistics ( Coling 2010), pages 313?321,
Beijing , August 2010
Mixture Model-based Minimum Bayes Risk Decoding using Multiple
Machine Translation Systems
Nan Duan1
School of Computer Science and Technology
Tianjin University
v-naduan@microsoft.com
Mu Li , Dongdong Zhang , Ming Zhou
Microsoft Research Asia
muli@microsoft.com
dozhang@microsoft.com
mingzhou@microsoft.com
Abstract
We present Mixture Model-based Minimum Bayes Risk ( MMMBR ) decoding , an approach that makes use of multiple
SMT systems to improve translation accuracy . Unlike existing MBR decoding methods defined on the basis of single
SMT systems , an MMMBR decoder reranks translation outputs in the combined search space of multiple systems using the MBR decision rule and a mixture distribution of component SMT models for translation hypotheses . MMMBR decoding is a general method that is independent of specific SMT models and can be applied to various commonly used search spaces . Experimental results on the NIST Chinese-to-English MT evaluation tasks show that our approach brings significant improvements to single system-based
MBR decoding and outperforms a state-of-the-art system combination method . 1 1 Introduction Minimum Bayes Risk ( MBR ) decoding is becoming more and more popular in recent Statistical Machine Translation ( SMT ) research . This approach requires a second-pass decoding procedure to rerank translation hypotheses by risk scores computed based on model?s distribution.
Kumar and Byrne (2004) first introduced
MBR decoding to SMT field and developed it on the Nbest list translations . Their work has shown that MBR decoding performs better than Maximum a Posteriori ( MAP ) decoding for different evaluation criteria . After that , many dedi-1 This work has been done while the author was visiting
Microsoft Research Asia.
cated efforts have been made to improve the performances of SMT systems by utilizing MBR-inspired methods . Tromble et al (2008) proposed a linear approximation to BLEU score ( log-BLEU ) as a new loss function in MBR decoding and extended it from Nbest lists to lattices , and Kumar et al (2009) presented more efficient algorithms for MBR decoding on both lattices and hypergraphs to alleviate the high computational cost problem in Tromble et al?s work . DeNero et al (2009) proposed a fast consensus decoding algorithm for MBR for both linear and nonlinear similarity measures.
All work mentioned above share a common setting : an MBR decoder is built based on one and only one MAP decoder . On the other hand , recent research has shown that substantial improvements can be achieved by utilizing consensus statistics over multiple SMT systems ( Rosti et al , 2007; Li et al , 2009a ; Li et al , 2009b ; Liu et al , 2009). It could be desirable to adapt MBR decoding to multiple SMT systems as well.
In this paper , we present Mixture Model-based Minimum Bayes Risk ( MMMBR ) decoding , an approach that makes use of multiple SMT systems to improve translation performance . In this work , we can take advantage of a larger search space for hypothesis selection , and employ an improved probability distribution over translation hypotheses based on mixture modeling , which linearly combines distributions of multiple component systems for Bayes risk computation . The key contribution of this paper is the usage of mixture modeling in MBR , which allows multiple SMT models to be involved in and makes the computation of ngram consensus statistics to be more accurate . Evaluation results have shown that our approach not only brings significant improvements to single system-based MBR decoding but also outperforms a state-of-the-art word-level system combination method.
313
The rest of the paper is organized as follows : In Section 2, we first review traditional MBR decoding method and summarize various search spaces that can be utilized by an MBR decoder.
Then , we describe how a mixture model can be used to combine distributions of multiple SMT systems for Bayes risk computation . Lastly , we present detailed MMMBR decoding model on multiple systems and make comparison with single system-based MBR decoding methods.
Section 3 describes how to optimize different types of parameters . Experimental results will be shown in Section 4. Section 5 discusses some related work and Section 6 concludes the paper.
2 Mixture Model-based MBR Decoding 2.1 Minimum Bayes Risk Decoding Given a source sentence , MBR decoding aims to find the translation with the least expected loss under a probability distribution . The objective of an MBR decoder can be written as : (1) where denotes a search space for hypothesis selection ; denotes an evidence space for Bayes risk computation ; denotes a function that measures the loss between and ; is the underlying distribution based on .
Some of existing work on MBR decoding focused on exploring larger spaces for both and , e.g . from Nbest lists to lattices or hypergraphs ( Tromble et al , 2008; Kumar et al , 2009). Various loss functions have also been investigated by using different evaluation criteria for similarity computation , e.g . Word Error Rate , Position-independent Word Error Rate , BLEU and log-BLEU ( Kumar and Byrne , 2004; Tromble et al , 2008). But less attention has been paid to distribution . Currently , many SMT systems based on different paradigms can yield similar performances but are good at modeling different inputs in the translation task ( Koehn et al , 2004a ; Och et al , 2004; Chiang , 2007; Mi et al , 2008; Huang , 2008). We expect to integrate the advantages of different SMT models into MBR decoding for further improvements . In particular , we make indepth investigation into MBR decoding concentrating on the translation distribution by leveraging a mixture model based on multiple SMT systems.
2.2 Summary of Translation Search Spaces There are three major forms of search spaces that can be obtained from an MAP decoder as a byproduct , depending on the design of the decoder : Nbest lists , lattices and hypergraphs.
An Nbest list contains the most probable translation hypotheses produced by a decoder . It only presents a very small portion of the entire search space of an SMT model.
A hypergraph is a weighted acyclic graph which compactly encodes an exponential number of translation hypotheses . It allows us to represent both phrasebased and syntaxbased systems in a unified framework . Formally , a hypergraph is a pair , where is a set of hypernodes and is a set of hyperedges.
Each hypernode corresponds to translation hypotheses with identical decoding states , which usually include the span of the words being translated , the grammar symbol for that span and the left and right boundary words of hypotheses for computing language model ( LM ) scores . Each hyperedge corresponds to a translation rule and connects a head node and a set of tail nodes . The number of tail nodes is called the arity of the hyperedge and the arity of a hypergraph is the maximum arity of its hyperedges . If the arity of a hyperedge is zero , is then called a source node . Each hypergraph has a unique root node and each path in a hypergraph induces a translation hypothesis . A lattice ( Ueffing et al , 2002) can be viewed as a special hypergraph , in which the maximum arity is one.
2.3 Mixture Model for SMT
We first describe how to construct a general distribution for translation hypotheses over multiple SMT systems using mixture modeling for usage in MBR decoding.
Mixture modeling is a technique that has been applied to many statistical tasks successfully.
For the SMT task in particular , given SMT systems with their corresponding model distributions , a mixture model is defined as a probability distribution over the combined search space of all component systems and computed as a weighted sum of component model distributions : (2) In Equation 2, are system weights which hold following constraints : and , is the th distribution estimated on the search space based on the loglinear formulation : where is the score function of the th system for translation , is a scaling factor that determines the flatness of the distribution sharp ( ) or smooth ( ).
Due to the inherent differences in SMT models , translation hypotheses have different distributions in different systems . A mixture model can effectively combine multiple distributions with tunable system weights . The distribution of a single model used in traditional MBR can be seen as a special mixture model , where is one.
2.4 Mixture Model for SMT
Let denote machine translation systems , denotes the search space produced by system in MAP decoding procedure . An MMMBR decoder aims to seek a translation from the combined search space that maximizes the expected gain score based on a mixture model . We write the objective function of MMMBR decoding as : (3) For the gain function , we follow Tromble et al (2008) to use log-BLEU , which is scored by the hypothesis length and a linear function of ngram matches as : In this definition , is a reference translation , is the length of hypothesis , is an ngram presented in , is the number of times that occurs in , and is an indicator function which equals to 1 when occurs in and 0 otherwise . are model parameters , where is the maximum order of the ngrams involved.
For the mixture model , we replace it by Equation 2 and rewrite the total gain score for hypothesis in Equation 3: (4) In Equation 4, the total gain score on the combined search space can be further decomposed into each local search space with a specified distribution . This is a nice property and it allows us to compute the total gain score as a weighted sum of local gain scores on different search spaces . We expand the local gain score for computed on search space with using log-BLEU as : We make two approximations for the situations when : the first is and the second is In fact , due to the differences in generative capabilities of SMT models , training data selection and various pruning techniques used , search spaces of different systems are always not identical in practice . For the convenience of formal analysis , we treat all as ideal distributions with assumptions that all systems work in similar settings , and translation candidates are shared by all systems.
The method for computing ngram posterior probability in Equation 5 depends on different types of search space : ? When is an Nbest list , it can be computed immediately by enumerating all translation candidates in the Nbest list : encodes exponential number of hypotheses , it is often impractical to compute this probability directly . In this paper , we use the algorithm presented in Kumar et al (2009) which is described in Algorithm 12: counts the edge with ngram that has the highest edge posterior probability relative to predecessors in the entire graph , and is the edge posterior probability that can be efficiently computed with standard inside and outside probabilities and as : where is the weight of hyperedge in , is the normalization factor that equals to the inside probability of the root node in .

Algorithm 1: Compute ngram posterior probabilities on hypergraph ( Kumar et al , 2009) 1: sort hypernodes topologically 2: compute inside/outside probabilities and for each hypernode 3: compute edge posterior probability for each hyperedge 4: for each hyperedge do 5: merge ngrams on and keep the highest probability when ngrams are duplicated 6: apply the rule of edge to ngrams on and propagate gram prefixes/suffixes to 7: for each ngram introduced by do 8: if then 9: 10: else 11: 12: end if 13: end for 14: end for 15: return ngram posterior probability set 2 We omit the similar algorithm for lattices because of their homogenous structures comparing to hypergraphs as we discussed in Section 2.2.
Thus , the total gain score for hypothesis on can be further expanded as : where is a mixture ngram posterior probability . The most important fact derived from Equation 6 is that , the mixture of different distributions can be simplified to the weighted sum of ngram posterior probabilities on different search spaces.
We now derive the decision rule of MMMBR decoding based on Equation 6 below : (7) We also notice that MAP decoding and MBR decoding are two different ways of estimating the probability and each of them has advantages and disadvantages . It is desirable to interpolate them together when choosing the final translation outputs . So we include each sys-tem?s MAP decoding cost as an additional feature further and modify Equation 7 to : ? (8) where is the model cost assigned by the MAP decoder for hypothesis .
Because the costs of MAP decoding on different SMT models are not directly comparable , we utilize the MERT algorithm to assign an appropriate weight for each component system.
Compared to single system-based MBR decoding , which obeys the decision rule below : tion ( Equation 8). The key difference is that , in MMMBR decoding , ngram posterior probability is computed as based on an ensemble of search spaces ; meanwhile , in single system-based MBR decoding , this quantity is computed locally on single search space .
The procedure of MMMBR decoding on multiple SMT systems is described in Algorithm 2.

Algorithm 2: MMMBR decoding on multiple
SMT systems 1: for each component system do 2: run MAP decoding and generate the corresponding search space 3: compute the ngram posterior probability set for based on Algorithm 1 4: end for 5 compute the mixture ngram posterior probability for each : 6: for each unique ngram appeared in do 7: for each search space do 10: end for 11: for each hyperedge in do 12: assign to the edge for all contained in 13: end for 14: return the best path according to Equation 8 3 A Two-Pass Parameter Optimization In Equation 8, there are two types of parameters : parameters introduced by the gain function and the model cost , and system weights introduced by the mixture model . Because Equation 8 is not a linear function when all parameters are taken into account , MERT algorithm ( Och , 2003) cannot be directly applied to optimize them at the same time . Our solution is to employ a two-pass training strategy , in which we optimize parameters for MBR first and then system weights for the mixture model.
3.1 Parameter Optimization for MBR
The inputs of an MMMBR decoder can be a combination of translation search spaces with arbitrary structures . For the sake of a general and convenience solution for optimization , we utilize the simplest Nbest lists with proper sizes as approximations to arbitrary search spaces to optimize MBR parameters using MERT in the first-pass training . System weights can be set empirically based on different performances , or equally without any bias . Note that although we tune MBR parameters on Nbest lists , ngram posterior probabilities used for Bayes risk computation could still be estimated on hypergraphs for non N-best-based search spaces.
3.2 Parameter Optimization for Mixture
Model
After MBR parameters optimized , we begin to tune system weights for the mixture model in the second-pass training . We rewrite Equation 8 as : ? For each , the aggregated score surrounded with braces can be seen as its feature value . Equation 9 now turns to be a linear function for all weights and can be optimized by the MERT.
4 Experiments 4.1 Data and Metric
We conduct experiments on the NIST Chinese-to-English machine translation tasks . We use the newswire portion of the NIST 2006 test set ( MT06-nw ) as the development set for parameter optimization , and report results on the NIST 2008 test set ( MT08). Translation performances are measured in terms of case-insensitive BLEU scores . Statistical significance is computed using the bootstrap resampling method proposed by Koehn (2004b ). Table 1 gives data statistics.

Data Set # Sentence # Word
MT06-nw ( dev ) 616 17,316
MT08 ( test ) 1,357 31,600
Table 1. Statistics on dev and test data sets All bilingual corpora available for the NIST 2008 constrained track of Chinese-to-English machine translation task are used as training data , which contain 5.1M sentence pairs , 128M Chinese words and 147M English words after preprocessing . Word alignments are performed by GIZA ++ with an intersect-diag-grow refinement.
317
A 5gram language model is trained on the English side of all bilingual data plus the Xinhua portion of LDC English Gigaword Version 3.0.
4.2 System Description
We use two baseline systems . The first one ( SYS1) is a hierarchical phrasebased system ( Chiang , 2007) based on Synchronous Context Free Grammar ( SCFG ), and the second one ( SYS2) is a phrasal system ( Xiong et al , 2006) based on Bracketing Transduction Grammar ( Wu , 1997) with a lexicalized reordering component based on maximum entropy model.
Phrasal rules shared by both systems are extracted on all bilingual data , while hierarchical rules for SYS1 only are extracted on a selected data set , including LDC2003E07, LDC2003E14,
LDC2005T06, LDC2005T10, LDC2005E83,
LDC2006E26, LDC2006E34, LDC2006E85 and
LDC2006E92, which contain about 498,000 sentence pairs . Translation hypergraphs are generated by each baseline system during the MAP decoding phase , and 1000-best lists used for MERT algorithm are extracted from hypergraphs by the kbest parsing algorithm ( Huang and Chiang , 2005). We tune scaling factor to optimize the performance of HyperGraph-based
MBR decoding ( HGMBR ) on MT06-nw for each system (0.5 for SYS1 and 0.01 for SYS2).
4.3 MMMBR Results on Multiple Systems
We first present the overall results of MMMBR decoding on two baseline systems.
To compare with single system-based MBR methods , we reimplement Nbest MBR , which performs MBR decoding on 1000-best lists with the fast consensus decoding algorithm ( DeNero et al , 2009), and HGMBR , which performs MBR decoding on a hypergraph ( Kumar et al , 2009). Both methods use log-BLEU as the loss function . We also compare our method with IHMM Word-Comb , a state-of-the-art word-level system combination approach based on incremental HMM alignment proposed by Li et al (2009b ). We report results of MMMBR decoding on both Nbest lists ( Nbest MMMBR ) and hypergraphs ( Hypergraph MMMBR ) of two baseline systems . As MBR decoding can be used for any SMT system , we also evaluate MBR-IHMM Word-Comb , which uses Nbest lists generated by HGMBR on each baseline systems.
The default beam size is set to 50 for MAP decoding and hypergraph generation . The setting of Nbest candidates used for ( MBR -) IHMM Word-Comb is the same as the one used in Li et al . (2009b ). The maximum order of ngrams involved in MBR model is set to 4. Table 2 shows the evaluation results.
MT06-nw MT08 SYS1 SYS2 SYS1 SYS2
MAP 38.1 37.1 28.5 28.0
Nbest MBR 38.3 37.4 29.0 28.1
HGMBR 38.3 37.5 29.1 28.3
IHMM
Word-Comb 39.1 29.3
MBR-IHMM
Word-Comb 39.3 29.7
Nbest
MMMBR 39.0* 29.4*
Hypergraph
MMMBR 39.4*+ 29.9*+
Table 2. MMMBR decoding on multiple systems (*: significantly better than HGMBR with ; +: significantly better than IHMM
Word-Comb with )
From Table 2 we can see that , compared to MAP decoding , Nbest MBR and HGMBR only improve the performance in a relative small range (+0.1~+0.6 BLEU ), while MMMBR decoding on multiple systems can yield significant improvements on both dev set (+0.9 BLEU on
Nbest MMMBR and +1.3 BLEU on Hypergraph MMMBR ) and test set (+0.9 BLEU on Nbest MMMBR and +1.4 BLEU on Hypergraph
MMMBR ); compared to IHMM Word-Comb,
Nbest MMMBR can achieve comparable results on both dev and test sets , while Hypergraphs MMMBR can achieve even better results (+0.3 BLEU on dev and +0.6 BLEU on test ); compared to MBR-IHMM Word-Comb , Hypergraph MMMBR can also obtain comparable results with tiny improvements (+0.1 BLEU on dev and +0.2 BLEU on test ). However , MBR-IHMM Word-Comb has ability to generate new hypotheses , while Hypergraph MMMBR only chooses translations from original search spaces.
We next evaluate performances of MMMBR decoding on hypergraphs generated by different beam size settings , and compare them to ( MBR -) size and HGMBR with the same beam size . We list the results of MAP decoding for comparison.
The comparative results on MT08 are shown in Figure 1, where X-axis is the size used for all methods each time , Y-axis is the BLEU score , MAP - and HGMBR - stand for MAP decoding and HGMBR decoding for the th system.

Figure 1. MMMBR vs . ( MBR -) IHMM Word-
Comb and HGMBR with different sizes
From Figure 1 we can see that , MMMBR decoding performs consistently better than both ( MBR -) IHMM Word-Comb and HGMBR on all sizes . The gains achieved are around +0.5
BLEU compared to IHMM Word-Comb , +0.2
BLEU compared to MBR-IHMM Word-Comb , and +0.8 BLEU compared to HGMBR . Compared to MAP decoding , the best result (30.1) is obtained when the size is 100, and the largest improvement (+1.4 BLEU ) is obtained when the size is 50. However , we did not observe significant improvement when the size is larger than 50.
We then setup an experiment to verify that the mixture model based on multiple distributions is more effective than any individual distributions for Bayes risk computation in MBR decoding.
We use Mix-HGMBR to denote MBR decoding performed on single hypergraph of each system in the meantime using a mixture model upon distributions of two systems for Bayes risk computation . We compare it with HGMBR and Hypergraph MMMBR and list results in Table 3.
MT08 SYS1 SYS2
HGMBR 29.1 28.3
Mix-HGMBR 29.4 28.9
Hypergraph MMMBR 29.9
Table 3. Performance of MBR decoding on different settings of search spaces and distributions It can be seen that based on the same search space , the performance of Mix-HGMBR is significantly better than that of HGMBR (+0.3/+0.6 BLEU on dev/test ). Yet the performance is still not as good as Hypergraph , which indicates the fact that the mixture model and the combination of search spaces are both helpful to MBR decoding , and the best choice is to use them together.
We also empirically investigate the impacts of different system weight settings upon the performances of Hypergraph MMMBR on dev set in Figure 2, where X-axis is the weight for SYS1, Y-axis is the BLEU score . The weight for SYS2 equals to as only two systems involved . The best evaluation result on dev set is achieved when the weight pair is set to 0.7/0.3 for SYS1/SYS2, which is also very close to the one trained automatically by the training strategy presented in Section 3.2. Although this training strategy can be processed repeatedly , the performance is stable after the 1st round finished.

Figure 2. Impacts of different system weights in the mixture model 4.4 MMMBR Results on Identical Systems with Different Translation Models Inspired by Macherey and Och (2007), we arrange a similar experiment to test MMMBR decoding for each baseline system on an ensemble of subsystems built by the following two steps.
Firstly , we iteratively apply the following procedure 3 times : at the th time , we randomly sample 80% sentence pairs from the total bilingual data to train a translation model and use it to build a new system based on the same decoder , which is denoted as subsystem - . Table 4 shows the evaluation results of all subsystems on MT08, where MAP decoding ( the former ones ) and corresponding HGMBR ( the latter ones ) are grouped together by a slash . We set al beam sizes to 20 for a time-saving purpose.
27.5 28.0 28.5 29.0 29.5 30.0 30.5 10 20 50 100 150
MAP1
MAP-2
HGMBR-1
HGMBR-2
IHMM
MBR-IHMM
MMMBR 38.5 38.7 38.9 39.1 39.3 39.5 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
MMMBR SYS1 SYS2
Baseline 28.4/29.0 27.6/27.8 sub-system-1 28.1/28.5 26.8/27.3 sub-system-2 28.3/28.4 27.0/27.1 sub-system-3 27.7/28.0 27.3/27.6
Table 4. Performance of subsystems
Secondly , starting from each baseline system , we gradually add one more subsystem each time and perform Hypergraph MMMBR on hypergraphs generated by current involved systems . Table 5 shows the evaluation results.
MT08 SYS1 SYS2
MAP 28.4 27.6
HGMBR 29.0 27.8
Hypergraph MMMBR + sub-system-1 29.1 27.9 + sub-system-2 29.1 28.1 + sub-system-3 29.3 28.3 Table 5. Performance of Hypergraph MMMBR on multiple subsystems We can see from Table 5 that , compared to the results of MAP decoding , MMMBR decoding can achieve significant improvements when more than one subsystem are involved ; however , compared to the results of HGMBR on baseline systems , there are few changes of performance when the number of subsystems increases . One potential reason is that the translation hypotheses between multiple subsystems under the same SMT model hold high degree of correlation , which is discussed in Macherey and Och (2007).
We also evaluate MBR-IHMM Word-Comb on Nbest lists generated by each baseline system with its corresponding three subsystems.
Evaluation results are shown in Table 6, where Hypergraph MMMBR still outperforms MBR-IHMM Word-Comb on both baseline systems.
MT08 SYS1 SYS2
MBR-IHMM Word-Comb 29.1 28.0
Hypergraph MMMBR 29.3 28.3
Table 6. Hypergraph MMMBR vs . MBR-IHMM
Word-Comb with multiple subsystems 5 Related Work Employing consensus between multiple systems to improve machine translation quality has made rapid progress in recent years . System combination methods based on confusion networks ( Rosti et al , 2007; Li et al , 2009b ) have shown state-of-the-art performances in MT benchmarks.
Different from them , MMMBR decoding method does not generate new translations . It maintains the essential of MBR methods to seek translations from existing search spaces . Hypothesis selection method ( Hildebrand and Vogel , 2008) resembles more our method in making use of ngram statistics . Yet their work does not belong to the MBR framework and treats all systems equally . Li et al (2009a ) presents a co-decoding method , in which ngram agreement and disagreement statistics between translations of multiple decoders are employed to rerank both full and partial hypotheses during decoding.
Liu et al (2009) proposes a joint-decoding method to combine multiple SMT models into one decoder and integrate translation hypergraphs generated by different models . Both of the last two methods work in a white-box way and need to implement a more complicated decoder to integrate multiple SMT models to work together ; meanwhile our method can be conveniently used as a second-pass decoding procedure , without considering any system implementation details.
6 Conclusions and Future Work
In this paper , we have presented a novel MMMBR decoding approach that makes use of a mixture distribution of multiple SMT systems to improve translation accuracy . Compared to single system-based MBR decoding methods , our method can achieve significant improvements on both dev and test sets . What is more , MMMBR decoding approach also outperforms a state-of-the-art system combination method . We have empirically verified that the success of our method comes from both the mixture modeling of translation hypotheses and the combined search space for translation selection.
In the future , we will include more SMT systems with more complicated models into our
MMMBR decoder and employ more general
MERT algorithms on hypergraphs and lattices ( Kumar et al , 2009) for parameter optimization.
320
References
Chiang David . 2007. Hierarchical Phrase Based Translation . Computational Linguistics , 33(2): 201-228.
DeNero John , David Chiang , and Kevin Knight . 2009.
Fast Consensus Decoding over Translation Forests . In Proc . of 47th Meeting of the Association for Computational Linguistics , pages 567-575.
Hildebrand Almut Silja and Stephan Vogel . 2008.
Combination of Machine Translation Systems via Hypothesis Selection from Combined Nbest lists . In Proc . of the Association for Machine Translation in the Americas , pages 254-261.
Huang Liang and David Chiang . 2005. Better kbest Parsing . In Proc . of 7th International Conference on Parsing Technologies , pages 53-64.
Huang Liang . 2008. Forest Reranking : Discriminative Parsing with Non-Local Features . In Proc . of 46th Meeting of the Association for Computational Linguistics , pages 586-594.
Koehn Philipp . 2004a . Phrase-based Model for SMT . Computational Linguistics , 28(1): 114-133.
Koehn Philipp . 2004b . Statistical Significance Tests for Machine Translation Evaluation . In Proc . of Empirical Methods on Natural Language
Processing , pages 388-395.
Kumar Shankar and William Byrne . 2004. Minimum Bayes-Risk Decoding for Statistical Machine Translation . In Proc . of the North American Chapter of the Association for Computational Linguistics , pages 169-176.
Kumar Shankar , Wolfgang Macherey , Chris Dyer , and Franz Och . 2009. Efficient Minimum Error Rate Training and Minimum Bayes-Risk Decoding for Translation Hypergraphs and Lattices . In Proc . of 47th Meeting of the Association for Computational Linguistics , pages 163-171.
Li Mu , Nan Duan , Dongdong Zhang , Chi-Ho Li , and Ming Zhou . 2009a . Collaborative Decoding : Partial Hypothesis ReRanking Using Translation Consensus between Decoders . In Proc.
of 47th Meeting of the Association for Computational Linguistics , pages 585-592.
Liu Yang , Haitao Mi , Yang Feng , and Qun Liu . 2009.
Joint Decoding with Multiple Translation Models . In Proc . of 47th Meeting of the Association for Computational Linguistics , pages 576-584.
Li Chi-Ho , Xiaodong He , Yupeng Liu , and Ning Xi.
2009b . Incremental HMM Alignment for MT system Combination . In Proc . of 47th Meeting of the Association for Computational Linguistics , pages 949-957.
Mi Haitao , Liang Huang , and Qun Liu . 2008. Forest-Based Translation . In Proc . of 46th Meeting of the Association for Computational Linguistics , pages 192-199.
Macherey Wolfgang and Franz Och . 2007. An Empirical Study on Computing Consensus Translations from multiple Machine Translation Systems . In Proc . of Empirical Methods on Natural Language Processing , pages 986-995.
Och Franz . 2003. Minimum Error Rate Training in Statistical Machine Translation . In Proc . of 41th Meeting of the Association for Computational
Linguistics , pages 160-167.
Och Franz and Hermann Ney . 2004. The Alignment template approach to Statistical Machine Translation . Computational Linguistics , 30(4): 417-449.
Rosti Antti-Veikko , Spyros Matsoukas , and Richard Schwartz . 2007. Improved Word-Level System Combination for Machine Translation . In Proc.
of 45th Meeting of the Association for Computational Linguistics , pages 312-319.
Roy Tromble , Shankar Kumar , Franz Och , and Wolfgang Macherey . 2008. Lattice Minimum Bayes-Risk Decoding for Statistical Machine Translation . In Proc . of Empirical Methods on Natural
Language Processing , pages 620-629.
Ueffing Nicola , Franz Och , and Hermann Ney . 2002.
Generation of Word Graphs in Statistical Machine Translation . In Proc . of Empirical Methods on Natural Language Processing , pages 156-163.
Wu Dekai . 1997. Stochastic Inversion Transduction Grammars and Bilingual Parsing of Parallel Corpora . Computational Linguistics , 23(3): 377-404.
Xiong Deyi , Qun Liu , and Shouxun Lin . 2006. Maximum Entropy based Phrase Reordering Model for Statistical Machine Translation . In Proc . of 44th Meeting of the Association for Computational Linguistics , pages 521-528.
321
